\section{An Introduction to Tensors and Tensor Networks}
\label{sec:tensors:tensors}
In this section, we introduce tensors and tensor networks and discuss prior work on the optimization of tensor-network contraction. To aid in exposition, along the way we build an analogy between the language of databases \cite{SG88}, the language of factor graphs \cite{KFL01,dechter99,darwiche01b}, and the language of tensors: see Table \ref{table:db-tensor-analogy}.

\begin{table}[t]
\centering
\begin{tabular}{c|c|c}
\hline
\textbf{Database Concept} & \textbf{Factor Graph Concept} & \textbf{Tensor Concept}\\ \hline
Attribute & Variable & Index\\
Table & Factor & Tensor\\
Project-Join Query & Factor Graph & Tensor Network\\
Join Tree & Dtree & Contraction Tree\\ 
Width & Largest Cluster Size & Contraction Complexity \\
- & Largest Separator Size & Max Rank \\ \hline
\end{tabular}
\caption{\label{table:db-tensor-analogy} An analogy between the language of databases, the language of factor graphs, and the language of tensors.}
\end{table}

\subsection{Tensors}
\emph{Tensors} are a generalization of vectors and matrices to higher dimensions-- a tensor with $r$ dimensions is a table of values each labeled by $r$ indices. An index is analogous to a variable in constraint satisfaction or an attribute in database theory. 

Fix a set $\Ind$ and define an \emph{index} to be an element of $\Ind$. For each index $i$ fix a finite set $\domain{i}$ called the \emph{domain} of $i$. An \emph{assignment} to a set of indices $I \subseteq \Ind$ is a function $\tau$ that maps each index $i \in I$ to an element of $\domain{i}$. Let $\domain{I}$ denote the set of assignments to $I$, i.e., $$\domain{I} = \{\tau: I \rightarrow \bigcup_{i \in I} \domain{i}~\text{s.t.}~\tau(i) \in \domain{i}~\text{for all}~i \in I\}.$$

% Formally, fix a finite set $\Ind$ and finite sets $\domain{i}$ for each $i \in \Ind$. An \emph{index} is an element $i$ of $\Ind$, and the corresponding set $\domain{i}$ is called the \emph{domain} of $i$. 
%Fix a finite set $\Ind$, whose elements we call \emph{indices}, and for each index fix a finite set $\domain{i}$ called the \emph{domain} of $i$. 

We now define tensors as multidimensional arrays of values, indexed by assignments to a set of indices:\footnote{In some works, a tensor is defined as a multilinear map and Definition \ref{def:tensor} would be its representation in a fixed basis.}
\begin{definition}[Tensor] \label{def:tensor}
	A \emph{tensor} $A$ over a finite set of indices (denoted $\tdim{A}$) is a function $A: \domain{\tdim{A}} \rightarrow \mathbb{C}$ (where $\mathbb{C}$ is the set of complex numbers).
\end{definition}

The \emph{rank} of a tensor $A$ is the cardinality of $\tdim{A}$. The memory to store a tensor (in a dense way) is exponential in the rank. For example, a scalar is a rank 0 tensor, a vector is a rank 1 tensor, and a matrix is a rank 2 tensor. An example of a higher-rank tensor is the \emph{copy tensor} on a set of indices $I$, which is the tensor $\copyt_I: \domain{I} \rightarrow \mathbb{C}$ such that, for all $\tau \in \domain{I}$, $\copyt_I(\tau) \equiv 1$ if $\tau$ is a constant function on $I$ and $\copyt_I(\tau) \equiv 0$ otherwise \cite{BCJ11}.

It is common to consider sets of tensors closed under contraction (see Section \ref{sec:tensors:tensors:tensor-networks}), e.g. tensors with entries in $\mathbb{R}$ as in Section \ref{sec:tensors:wmc}. Database tables under bag-semantics \cite{CV93}, i.e., multirelations, are tensors with entries in $\mathbb{N}$. Probabilistic database tables \cite{CP87} are tensors with entries in $[0, 1]$ that sum to 1.

Many tools exist (e.g. \pkg{numpy} \cite{numpy}) to efficiently manipulate tensors. In Section \ref{sec:tensors:experiments}, we use these tools to implement tensor-network contraction, defined next.

% We abuse notation and allow for two tensors to be treated as distinct even if they are equal as functions.

%Given two tensors with possibly overlapping sets of indices, a natural operation is the contraction of the two tensors.
%\begin{definition}[Tensor Contraction]
%	Let $A$ and $B$ be tensors. The \emph{contraction} of $A$ and $B$, denoted $A \cdot B$, is the tensor $A \cdot B: \domain{\tdim{A} \oplus \tdim{B}} \rightarrow \mathbb{C}$ defined by
%	$$\tau \mapsto \sum_{\rho \in \domain{\tdim{A} \cap \tdim{B}}} A(\rho \cup \tau\restrict{\tdim{A}}) \cdot B(\rho \cup \tau\restrict{\tdim{B}}).$$
%\end{definition}
% For example, given two matrices represented as rank 2 tensors with a single index in common, the contraction of the two tensors is equivalent to matrix multiplication. The contraction of two copy tensors $\copyt_I$ and $\copyt_J$ (with $I \cap J \neq \emptyset$) is the copy tensor $\copyt_{I \oplus J}$.

\subsection{Tensor Networks}
\label{sec:tensors:tensors:tensor-networks}
A \emph{tensor network} defines a complex tensor by combining a set of simpler tensors in a principled way. This is analogous to how a database query defines a resulting table in terms of a computation across many tables.

\begin{definition}[Tensor Network]
	\label{def:tensor-contraction-network}
	A \emph{tensor network} $N$ is a nonempty set of tensors across which no index appears more than twice.
\end{definition}

\emph{Free indices} of $N$ are indices that appear once, while \emph{bond indices} of $N$ are indices that appear twice. We denote the set of free indices of $N$ by $\tnfree{N}$ and the set of bond indices of $N$ by $\tnbound{N}$. The \emph{bond dimension} of $N$ is the maximum size of $\domain{i}$ for all bond indices $i$ of $N$.

The problem of \emph{tensor-network contraction}, given an input tensor network $N$, is to compute the \emph{contraction} of $N$ by marginalizing all bond indices:
\begin{definition}[Tensor Network Contraction]\label{def:contraction}
The \emph{contraction} of a tensor network $N$ is a tensor $\tntensor{N}$ with indices $\tnfree{N}$ (the set of free indices of $N$), i.e. a function $\tntensor{N} : \domain{\tnfree{N}} \rightarrow \mathbb{C}$, that is defined for all $\tau \in \domain{\tnfree{N}}$ by
		\begin{equation}
        \label{eqn:contraction} 
        \tntensor{N}(\tau) \equiv \sum_{\rho \in \domain{\tnbound{N}}} \prod_{A \in N} A((\rho \cup \tau)\restrict{\tdim{A}}).
        \end{equation}
\end{definition}

For example, the contraction of the tensor network $\{\copyt_I, \copyt_J\}$ with $I \cap J \neq \emptyset$ is the tensor $\copyt_{I \oplus J}$ (where $I \oplus J$ is the symmetric difference of $I$ and $J$). Notice that if a tensor network has no free indices then its contraction is a rank 0 tensor. We write $A \cdot B$ to mean the contraction of the tensor network containing the two tensors $A$ and $B$. 

Following our analogy, given a tensor network containing database tables (under bag-semantics) as tensors, its contraction is the join of those tables followed by the projection of all shared attributes. Thus a tensor network is analogous to a project-join query. A tensor network can also be seen as a variant of a factor graph \cite{KFL01} with the additional practical restriction that no variable appears more than twice. The contraction of a tensor network corresponds to the marginalization of a factor graph \cite{RS17} and can similarly be seen as a special case of the FAQ problem \cite{KNR16}. The restriction on the appearance of variables is heavily exploited in tools for tensor-network contraction, since it allows tensor contraction to be implemented as matrix multiplication and thus leverage significant work in high-performance computing on matrix multiplication, both on the CPU \cite{LHKK77} and the GPU \cite{FSH04}.

We focus in this work on tensor networks with relatively few (or no) free indices and hundreds or thousands of bond indices. Such tensor networks are obtained in a variety of applications \cite{Cichocki14,DLVR18}, including the reduction from model counting to tensor network contraction \cite{BMT15}. Although the rank of the contraction of the tensor network $N$ is small in this case, computing entries of $\tntensor{N}$ by
directly following Equation \ref{eqn:contraction} requires performing a summation over an exponential number of terms--- one for each assignment in $\domain{\tnbound{N}}$--- and is therefore infeasible.

$\tntensor{N}$ can instead be computed by recursively decomposing the tensor network, as in Algorithm \ref{alg:network-contraction} \cite{EP14}. The choice of rooted binary tree $T$ does not affect the output of Algorithm \ref{alg:network-contraction} but may have a dramatic impact on the running-time and memory usage. We explore this in more detail in the following section.

\begin{algorithm}[t]
	\caption{Recursively contracting a tensor network}\label{alg:network-contraction}
	\hspace*{\algorithmicindent} \textbf{Input:} A tensor network $N$ and a rooted binary tree $T$ whose leaves are the tensors of $N$, i.e. $\Lv{T} = N$. \\
	\hspace*{\algorithmicindent} \textbf{Output:} $\tntensor{N}$, the contraction of $N$ as given in Definition \ref{def:contraction}.
	\begin{algorithmic}[1]
	    \Procedure{Contract}{$N,T$}
		\If {$\left|N\right| = 1$}
		\State \Return the tensor contained in $N$
		\Else
        \State $T_1, T_2 \gets \text{immediate subtrees of}~T$
		\State $A_1 \gets \Call{Contract}{\Lv{T_1}, T_1}$
		\State $A_2 \gets \Call{Contract}{\Lv{T_2}, T_2}$
		\State \Return $A_1 \cdot A_2$
		\EndIf
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\subsection{Contracting Tensor Networks}
The rooted binary trees used by Algorithm \ref{alg:network-contraction} are called contraction trees:
\begin{definition}[Contraction Tree \cite{EP14}] \label{def:contraction-tree}
	Let $N$ be a tensor network. A \emph{contraction tree} for $N$ is a rooted binary tree $T$ whose leaves are the tensors of $N$. %, i.e. $\Lv{T} = N$.
\end{definition}

In our database analogy, a contraction tree for a tensor network representing a project-join query is a join tree of that query (with projections done as early as possible). In our factor-graph analogy, a contraction tree corresponds to a \emph{dtree} \cite{darwiche01},
%MYV{Hard to parse next sentence fragment.} %JMD: Adjusted
to an elimination order \cite{darwiche01b}, 
and to a binary join tree \cite{shenoy97} where factors are assigned to leaf nodes.

The problem of \emph{tensor-network-contraction optimization}, which we tackle in this paper, is given a tensor network $N$ to find a contraction tree that minimizes the computational cost of Algorithm \ref{alg:network-contraction}. % Prior work can be categorized into two types: \emph{exact} algorithms that focus on finding the best possible contraction tree, and \emph{heuristic} algorithms that aim to find a ``good-enough'' contraction tree. Since the number of possible contraction trees is exponential in the number of tensors, exact algorithms are generally best for tensor networks with few tensors while heuristic algorithms scale to larger tensor networks but offer worse contraction trees.
Several \emph{cost-based} approaches aim for minimizing the total number of floating point operations to perform Algorithm \ref{alg:network-contraction} in step 8, e.g. \cite{PHV14} and the \pkg{einsum} package in \pkg{numpy}.  % A more complex cost-based approach is the Tensor Contraction Engine \cite{Hirata03}, which performs additional optimizations of Algorithm \ref{alg:network-contraction} at the compiler level. 
In this work, we instead focus on \emph{structure-based} approaches to tensor-network-contraction optimization, which analyze the rank of intermediate tensors that appear during Algorithm \ref{alg:network-contraction}. These ranks indicate the amount of memory and computation required at each recursive stage. Moreover, these ranks are more amenable to analysis.

One line of work \cite{MS08,DFGHSW18} uses graph decompositions to analyze the \emph{contraction complexity} of a contraction tree: the maximum over all recursive calls of the sum (minus 1) of the rank of the two tensors contracted in step 8 of Algorithm \ref{alg:network-contraction}. In our factor graph analogy, contraction complexity of a contraction tree corresponds to the \emph{width} of a dtree (i.e., the size of the largest cluster) \cite{darwiche01b}. Contraction complexity measures the memory required when step 8 is computed
by summing over the shared indices separately, i.e., first contracting $A_1$ and $A_2$ as if they have no common indices (producing and storing a rank $|\tdim{A_1}|+|\tdim{A_2}|$ tensor) and then sequentially contracting each pair of shared indices. In this approach, the memory required is exponential in the contraction complexity. Modern tensor packages (e.g. \pkg{numpy}), however, instead sum over all shared indices simultaneously without storing an intermediate result. This alternative approach requires the same number of floating-point operations, but often requires significantly less intermediate memory. Thus contraction complexity overestimates the memory requirements of many contraction trees. 

Instead, another line of structure-based optimization analyzes the maximum rank over all recursive calls of the result of step 8 (and step 3). We call this the \emph{max rank} of a contraction tree. In our factor graph analogy, max rank of a contraction tree corresponds to the size of the largest separator of a dtree \cite{darwiche01b} and is closely connected to the performance of the Shenoy-Shafer algorithm \cite{shenoy97,SS08}. Max rank measures the memory required when step 8 is computed by summing over all shared indices simultaneously without storing an intermediate result: the memory required is exponential in the max rank. Thus, max rank estimates the memory usage of modern tensor packages. While the max rank is no more than linearly smaller than the contraction complexity, both appear in the exponent of the required memory. Therefore even small differences between max rank and contraction complexity result in significantly different memory usage. Recent work \cite{KCMR18} introduced three methods for heuristically minimizing the max rank: a greedy approach (called \textbf{greedy}), an approach using graph partitioning (called \textbf{metis}), and an approach using community-structure detection (called \textbf{GN}). Note that \textbf{metis} is analogous to earlier work on using graph partitioning to construct dtrees with small separators \cite{darwiche01b}.

In this work, we improve on these methods by using graph decompositions to find contraction trees with small max rank.

