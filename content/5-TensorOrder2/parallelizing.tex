\section{Parallelizing Tensor-Network Contraction}
\label{sec:parallel}

In this section, we discuss opportunities for parallelization of Algorithm \ref{alg:wmc}. Since the reduction stage was not a significant source of runtime in \cite{DDV19}, we focus on opportunities in the planning and execution stages.

\subsection{Parallelizing the Planning Stage with a Portfolio}
\label{sec:parallel:planning}
As discussed in Section \ref{sec:tensors:preprocessing}, $\func{FactorTree}(N)$ first computes a tree decomposition of $\struct{N}$ and then applies Theorem \ref{thm:factorable-tree}. In \cite{DDV19}, most time in the planning stage was spent finding low-width tree decompositions with a choice of single-core heuristic tree-decomposition solvers \cite{AMW17,HS18,Tamaki17}. Finding low-width tree decompositions is thus a valuable target for parallelization.

We were unable to find state-of-the-art parallel heuristic tools for tree decompositions. There are several classic algorithms for parallel tree-decomposition construction \cite{Lagergren90,SWG13}, but no recent implementations. There is also an exact tree-decomposition solver that leverages a GPU \cite{VB17}, but exact tools are not able to scale to handle our benchmarks. Existing single-core heuristic tree-decomposition solvers are highly optimized and so parallelizing them is nontrivial.

Instead, we take inspiration from the SAT solving community. One competitive approach to building a parallel SAT solver is to run a variety of SAT solvers in parallel across multiple cores \cite{BSS15,MSSS13,XHHL08}. The portfolio SAT solver \tool{CSHCpar} \cite{MSSS13} won the open parallel track in the 2013 SAT Competition with this technique, and such portfolio solvers are now banned from most tracks due to their effectiveness relative to their ease of implementation. Similar parallel portfolios are thus promising for integration into the planning stage.

We apply this technique to heuristic tree-decomposition solvers by running a variety of single-core heuristic tree-decomposition solvers in parallel on multiple cores and collating their outputs into a single stream. We analyze the performance of this technique in Section \ref{sec:experiments}.

While Theorem \ref{thm:factorable-tree} lets us integrate tree-decomposition solvers into the portfolio, the portfolio might also be improved by integrating solvers of other graph decompositions. The following theorem shows that branch-decomposition solvers may also be used for $\func{FactorTree}(N)$:
\begin{theorem} \label{thm:factorable-branch}
Let $N$ be a tensor network of tree-factorable tensors s.t. $|\tnfree{N}| \leq 3$ and $G=\struct{N}$ has a branch decomposition $T$ of width $w \geq 1$. Then we can construct in polynomial time a tensor network $M$ and a contraction tree $S$ for $M$ s.t. $\text{max-rank}(S) \leq \ceil{4w/3}$ and $N$ is a partial contraction of $M$.
\end{theorem}
\begin{proof}
This proof differs from the proof of Theorem \ref{thm:factorable-tree} in \cite{DDV19} only in Part 1 and Part 4 (and in the definition of $\rho$ in Part 3).

The proof proceeds in five steps: (1) compute the factored tensor network $M$, (2) construct a graph $H$ that is a simplified version of the structure graph of $M$, (3) construct a carving decomposition $S$ of $H$, (4) bound the width of $S$, and (5) use $S$ to find a contraction tree for $M$. Working with $H$ instead of directly working with the structure graph of $M$ allows us to cleanly handle tensor networks with free indices.

\textbf{Part 1: Factoring the network.}
Next, for each $v \in \V{G}$, define $T_v$ to be the smallest connected component of $T$ containing $\vinc{G}{v} \subseteq \Lv{T}$. Consider each $A \in N$. If $\tnfree{A} = \emptyset$, let $N_A = \{N_A\}$. Otherwise, observe that $T_A$ is a dimension tree of $A$ and so we can factor $A$ with $T_A$ using Definition \ref{def:tree-factorable} to get a tensor network $N_A$ and a bijection $g_A: \V{T_A} \rightarrow N_A$. Define $M = \cup_{A \in N} N_A$ and let $G'$ be the structure graph of $M$ with free vertex $\fv'$. The remainder of the proof is devoted to bounding the carving width of $G'$. To do this, it is helpful to define $\rho: \V{T} \rightarrow \V{G}$ by $\rho(n) = \{ v \in \V{G} : n \in \V{T_v}, |\vinc{T_v}{n}| = 3\}$. Note that $|\rho(n)| \leq w$ for all $n \in \V{T}$.

\textbf{Part 2: Constructing a simplified structure graph of $M$.} In order to easily characterize $G'$, we define a new, closely-related graph $H$ by taking a copy of $T_v$ for each $v \in \V{G}$ and connecting these copies where indicated by $g$. Formally, the vertices of $H$ are $\{(v, n) : v \in \V{G}, n \in \V{T_v}\}$. For every $v \in \V{G}$ and every arc in $T$ with endpoints $n, m \in \V{T_v}$, we add an edge between $(v, n)$ and $(v, m)$. Moreover, for each $e \in \E{G}$ incident to $v, w \in \V{G}$, we add an edge between $(v, g(e))$ and $(w, g(e))$. 

We will prove in Part 5 that the carving width of $G'$ is bounded from above by the carving width of $H$. We therefore focus in Part 3 and Part 4 on bounding the carving width of $H$. It is helpful for this to define the two projections $\pi_G : \V{H} \rightarrow \V{G}$ and $\pi_T : \V{H} \rightarrow \V{T}$ that indicate respectively the first or second component of a vertex of $H$. 
% For each $v \in \V{G}$, define $H_v = \pi_G^{-1}(v)$. Thus the sets $H_v$ form a partition of $\V{H}$. 

% This ensures that $H \cap H_v$ is isomorphic to $T_v$ and so $H \cap H_v$ is a tree with $|\vinc{G}{v}|$ leaves.

% The map $f: \E{G} \rightarrow \E{H}$ constructed in this way is an injection and satisfies property 3 above. Moreover, since $g(\vinc{G}{v})$ is exactly the leaves of $T_v$, each leaf $\ell \in \Lv{H \cap H_v}$ is incident to exactly one edge in the range of $f$, namely $f(g^{-1}(\ell))$.

\textbf{Part 3. Constructing a carving decomposition $S$ of $H$.}
The idea of the construction is, for each $n \in \V{T}$, to attach the elements of $\pi_T^{-1}(n)$ as leaves along the arcs incident to $n$. To do this, for every leaf node $\ell \in \Lv{T}$ with incident arc $a \in \vinc{T}{\ell}$ define $H_{\ell, a} = \pi_T^{-1}(\ell)$. For every non-leaf node $n \in \V{T} \setminus \Lv{T}$ partition $\pi_T^{-1}(n)$ into three sets $\{H_{n,a} : a \in \vinc{T}{n}\}$, ensuring that the degree 3 vertices are divided evenly (the degree 1 and 2 vertices can be placed arbitrarily). Observe that $\{H_{n,a} : n \in \V{T}, a \in \vinc{T}{n}\}$ is a partition of $\V{H}$, and there are at most $\ceil{|\rho(n)|/3}$ vertices of degree 3 in each $H_{n,a}$, since there are exactly $|\rho(n)|$ vertices of degree 3 in $\pi_T^{-1}(n)$. 

We use this to construct a carving decomposition $S$ from $T$ by adding each element of $H_{n,a}$ as a leaf along the arc $a$. Formally, let $x_v$ denote a fresh vertex for each $v \in \V{H}$, let $y_n$ denote a fresh vertex for each $n \in \V{T}$, and let $z_{n,a}$ denote a fresh vertex for each $n \in \V{T}$ and $a \in \vinc{T}{n}$. Define $\V{S}$ to be the union of $\V{H}$ with the set of these free vertices. 

We add an arc between $v$ and $x_v$ for every $v \in \V{H}$. Moreover, for every $a \in \E{T}$ with endpoints $o, p \in \einc{T}{a}$ add an arc between $y_{o,a}$ and $y_{p,a}$. For every $n \in \V{T}$ and incident arc $a \in \vinc{T}{n}$, construct an arbitrary sequence $I_{n,a}$ from $\{x_v : v \in H_{n,a}\}$. If $H_{n,a} = \emptyset$ then add an arc between $y_n$ and $z_{n,a}$. Otherwise, add arcs between $y_n$ and the first element of $I$, between consecutive elements of $I_{n,a}$, and between the last element of $I_{n,a}$ and $z_{n,a}$. 

Finally, remove the previous leaves of $T$ from $S$. The resulting tree $S$ is a carving decomposition of $H$, since we have added all vertices of $H$ as leaves and removed the previous leaves of $T$.

\textbf{Part 4: Computing the width of $S$.} In this part, we separately bound the width of the partition induced by each of the three kinds of arcs in $S$.

First, consider an arc $b$ between some $v \in \V{H}$ and $x_v$. Since all vertices of $H$ are degree 3 or smaller, $b$ defines a partition of width at most $3 \leq \ceil{4w/3}$.

Next, consider an arc $c_a$ between $z_{o,a}$ and $z_{p,a}$ for some arc $a \in \E{T}$ with endpoints $o, p \in \einc{T}{a}$.
Observe that removing $a$ from $T$ defines a partition $\{B_o, B_p\}$ of $\V{T}$, denoted so that $o \in B_o$ and $p \in B_p$. 

Then removing $c_a$ from $S$ defines the partition $\{ \pi_T^{-1}(B_o), \pi_T^{-1}(B_p) \}$ of $\Lv{S}$. By construction of $H$, all edges between $\pi_T^{-1}(B_o)$ and $\pi_T^{-1}(B_p)$ are between $\pi_T^{-1}(o)$ and $\pi_T^{-1}(p)$. Observe that every edge $e \in \E{H}$ between $\pi_T^{-1}(o)$ and $\pi_T^{-1}(p)$ corresponds under $g_v$ to $a$ in $T_v$ for some $v$. It follows that the number of edges between $\pi_T^{-1}(o)$ and $\pi_T^{-1}(o)$ is exactly the number of vertices in $G$ that are endpoints of edges in both $C_a$ and $\E{G} \setminus C_a$, which is bounded by $w$. Thus the partition defined by $c_a$ has width no larger than $w$. 

Finally, consider an arc $d$ added as one of the sequence of $|H_{n,a}|+1$ arcs between $y_n$, $I_{n,a}$, and $z_{n,a}$ for some $n \in \V{T}$ and $a \in \vinc{T}{n}$. Some elements of $H_{n,a}$ have changed blocks from the partition defined by $c_a$. Each vertex of degree 2 that changes blocks does not affect the width of the partition, but each vertex of degree 3 that changes blocks increases the width by 1. There are at most $\ceil{w/3}$ elements of degree 3 added as leaves between $y_n$ and $z_{n,a}$. Thus the partition defined by $d$ has width at most $w + \ceil{w/3} = \ceil{4w/3}$.

It follows that the width of $S$ is at most $\ceil{4w/3}$.

\textbf{Part 5: Bounding the max-rank of $M$.} Let $\fv$ be the free vertex of the structure graph of $N$. We first construct a new graph $H'$ from $H$ by, if $\tnfree{N} \neq \emptyset$, contracting all vertices in $\pi_G^{-1}(\fv)$ to a single vertex $\fv$. If $\tnfree{N} = \emptyset$, instead add $\fv$ as a fresh degree 0 vertex to $H'$. Moreover, for all $A \in N$ with $\tdim{A} = \emptyset$ add $A$ as a degree 0 vertex to $H'$. 

Note that adding degree 0 vertices to a graph does not affect the carving width. Moreover, since $|\tnfree{N}| \leq 3$ all vertices (except at most one) of $\pi_G^{-1}(\fv)$ are degree 2 or smaller. It follows that contracting $\pi_G^{-1}(\fv)$ does not increase the carving width. Thus the carving width of $H'$ is at most $\ceil{4w/3}$.

Moreover, $H'$ and $G'$ are isomorphic. To prove this, define an isomorphism $\phi: \V{H'} \rightarrow \V{G'}$ between $H'$ and $G'$ by, for all $v \in \V{H'}$:
$$\phi(v) \equiv \begin{cases}v&\text{if}~v \in N~\text{and}~\tdim{v}=\emptyset\\\fv'&v=\fv\\g_{\pi_G(v)}(\pi_T(v))&\text{if}~v \in \V{H}~\text{and}~\pi_G(v) \in N\end{cases}$$
$\phi$ is indeed an isomorphism between $H'$ and $G'$ because the functions $g_A$ are all isomorphisms and because an edge exists between $\pi_G^{-1}(v)$ and $\pi_G^{-1}(w)$ for $v, w \in \V{G}$ if and only if there is an edge between $v$ and $w$ in $G$. Thus the carving width of $G'$ is at most $\ceil{4w/3}$. By Theorem \ref{thm:contraction-equiv-carving}, then, $M$ has a contraction tree of max rank no larger than $\ceil{4w/3}$.
\end{proof}

Theorem \ref{thm:factorable-branch} subsumes Theorem \ref{thm:factorable-tree}, since given a graph $G$ and a tree decomposition for $G$ of width $w+1$ one can construct in polynomial time a branch decomposition for $G$ of width $w$ \cite{RS91}. Moreover, on many graphs there are branch decompositions whose width is smaller than all tree decompositions. We explore this potential in practice in Section \ref{sec:experiments}.

It was previously known that branch decompositions can also be used in variable elimination \cite{BDP09}, but Theorem \ref{thm:factorable-branch} considers branch decompositions of a different graph. For example, consider again computing the model count of $\psi = \left(\lor_{i=1}^n x_i\right) \land \left(\lor_{i=1}^n \neg x_i\right)$. Theorem \ref{thm:factorable-branch} uses branch decompositions of the incidence graph of $\psi$, which has branchwidth 2. Variable elimination uses branch decompositions of the primal graph of $\psi$, which has branchwidth $n-1$. 

\subsection{Parallelizing the Execution Stage with TensorFlow and Slicing}
\label{sec:parallelizing:execution}
As discussed in Chapter \ref{ch:background}, each contraction in Algorithm \ref{alg:network-contraction} can be implemented as a matrix multiplication. % TODO: fix
This was done in \cite{DDV19} using \tool{numpy} \cite{numpy}, and it is straightforward to adjust the implementation to leverage multiple cores with \tool{numpy} and a GPU with \tool{TensorFlow} \cite{ABCCDDDGII16}.

The primary challenge that emerges when using a GPU is dealing with the small onboard memory. For example, the \tool{NVIDIA Tesla-V100} (which we use in Section \ref{sec:experiments}) has just 16GB of onboard memory. This limits the size of tensors that can be easily manipulated. A single contraction of two tensors can be performed across multiple GPU kernel calls \cite{RRBSKH08}, and similar techniques were implemented in \tool{gpusat2} \cite{FHZ19}. These techniques, however, require parts of the input tensors to be copied into and out of GPU memory, which incurs significant slowdown.

Instead, we use \emph{index slicing} \cite{CZHNS18,GK20,VBNHRBM19} of a tensor network $N$. This technique is analogous to \emph{conditioning} on Bayesian networks \cite{darwiche01,dechter99,pearl86,SAS94}. The idea is to represent $\tntensor{N}$ as the sum of contractions of smaller tensor networks. Each smaller network contains tensor slices:

%The main it is to contract a tensor network $N$ by choosing a subset of bond indices $I \subseteq \tnbound{N}$ and representing $\tntensor{N}$ into the sum of $|\domain{I}|$ contractions of smaller tensor networks. % This corresponds to moving the marginalization of the indices in $I$ to the outside of Equation \ref{eqn:contraction}.
%These smaller tensor networks contain tensor slices:
\begin{definition}
Let $A$ be a tensor, $I \subseteq \textbf{Ind}$, and $\eta \in \domain{I}$. Then the \emph{$\eta$-slice} of $A$ is the tensor $A[\eta]$ with indices $\tdim{A} \setminus I$ defined for all $\tau \in \domain{\tdim{A} \setminus I}$ by $A[\eta](\tau) \equiv A((\tau \cup \eta)\restrict{\tdim{A}}).$
\end{definition}

These are called slices because every value in $A$ appears in exactly one tensor in $\{ A[\eta] : \eta \in \domain{I} \}$. We now define and prove the correctness of index slicing: 
\begin{theorem}
Let $N$ be a tensor network and let $I \subseteq \tnbound{N}$. For each $\eta \in \domain{I}$, let $N[\eta] = \{ A[\eta] : A \in N\}$. Then $\tntensor{N} = \sum_{\eta \in \domain{I}} \tntensor{N[\eta]}.$
\end{theorem}
\begin{proof}
Move the summation over $\eta \in \domain{I}$ to the outside of Equation \ref{eqn:contraction}, then apply the definition of tensor slices and recombine terms.
\end{proof}

\begin{algorithm*}[t]
    \label{alg:tn-sliced}
    \caption{Sliced contraction of a tensor network}
    \DontPrintSemicolon
    \KwIn{$N$: a tensor network}
    \KwIn{$T$: a contraction tree for $N$}
    \KwIn{$m$: a memory bound}
    \KwOut{$\tntensor{N}$, the contraction of $N$, performed using at most $m$ memory.}
    \Function{\upshape $\func{ExecuteSliced}(N, T, m)$}{
        $I \gets \emptyset$\;
        \While{$\func{MemCost}(N, T, I) > m$}{
            $I \gets I \cup \{\func{ChooseSliceIndex}(N, T, I)\}$\;
        }
        \Return{$\sum_{\eta \in [I]} \func{Execute}(N[\eta], T[\eta])$}
    }
\end{algorithm*}

By choosing $I$ carefully, computing each $\tntensor{N[\eta]}$ uses less intermediate memory (compared to computing $\tntensor{N}$) while using the same contraction tree. In exchange, the number of floating point operations to compute all $\tntensor{N[\eta]}$ terms increases. 

Choosing $I$ is itself a difficult problem. Our goal is to choose the smallest $I$ so that contracting each network slice $N[\eta]$ can be done in onboard memory. We first consider adapting Bayesian network conditioning heuristics to the context of tensor networks. Two popular conditioning heuristics are (1) \emph{cutset conditioning} \cite{pearl86}, which chooses $I$ so that each network slice is a tree, and (2) \emph{recursive conditioning} \cite{darwiche01}, which chooses $I$ so that each network slice is disconnected\footnote{The full recursive conditioning procedure then recurses on each connected component. While recursive conditioning is an any-space algorithm, the partial caching required for this is difficult to implement on a GPU.}. Both of these heuristics result in a choice of $I$ far larger than our goal requires. 
% Cutset conditioning chooses $I$ much larger than needed for our purpose, while recursive conditioning scales in memory using an explicit 


Instead, in this work as a first step we use a heuristic from \cite{CZHNS18,GK20}: choose $I$ incrementally, greedily minimizing the memory cost of contracting $N[\eta]$ until the memory usage fits in onboard memory. Unlike cutset and recursive conditioning, the resulting networks $N[\eta]$ are typically still highly connected. One direction for future work is to compare other heuristics for choosing $I$ (e.g., see the discussion in Section 10 of \cite{dechter99}).

This gives us Algorithm \ref{alg:tn-sliced}, which performs the execution stage with limited memory at the cost of additional time. $T[\eta]$ is the contraction tree obtained by computing the $\eta$-slice of every tensor in $T$. $\func{MemCost}(N, T, I)$ computes the memory for one $\func{Execute}(N[\eta], T[\eta])$ call. $\func{ChooseSliceIndex}(N,T,I)$ chooses the next slice index greedily to minimize memory cost.

%\begin{align*}
%    \tntensor{N}(\tau) =& \sum_{\rho \in \domain{\tnbound{N}}} \prod_{A \in N} A((\rho \cup \tau)\restrict{\tdim{A}}) \\
%    =& \sum_{\eta \in \domain{I}} \sum_{\rho \in \domain{\tnbound{N} \setminus I}} \prod_{A \in N} A((\eta \cup \rho \cup \tau)\restrict{\tdim{A}}) \\
%    =& \sum_{\eta \in \domain{I}} \left( \sum_{\rho \in \domain{\tnbound{N} \setminus I}} \prod_{A \in N} A[\eta]((\rho \cup \tau)\restrict{\tdim{A}}) \right) \\
%    =& \sum_{\eta \in \domain{I}} \tntensor{\{A[\eta] : A \in N\}}.
%\end{align*}
