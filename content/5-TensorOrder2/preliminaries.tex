\section{Preliminaries}
\label{sec:prelim}

In this section, we give background information on the three concepts we combine in this paper: weighted model counting, graph decompositions, and tensor-network contraction. % We also choose notations, generally following \cite{DDV19}.

\subsection{Literal-Weighted Model Counting}
\label{sec:wmc}
The task in weighted model counting is to count the total weight, subject to a given weight function, of the set of solutions of input constraints (typically given in CNF). We focus on so-called \emph{literal-weight functions}, where the weight of a solution can be expressed as the product of weights associated with all satisfied literals. Formally:
\begin{definition}[Weighted Model Count]
  Let $\varphi$ be a formula over Boolean variables $X$ and let $W: X \times \{0,1\} \rightarrow \mathbb{R}$ be a function. The \emph{(literal-)weighted model count} of $\varphi$ w.r.t. $W$ is
  $W(\varphi) \equiv \sum_{\tau \in \domain{X}} \varphi(\tau) \cdot \prod_{x \in X} W(x, \tau(x)),$ where $[X]$ is the set of all functions from $X$ to $\{0, 1\}$.
\end{definition}

We focus in this work on weighted model counting, as opposed to \emph{unweighted model counting} where the weight function $W$ is constant. There are a variety of counters \cite{CW16,FHMW17,Thurley2006} that can perform only unweighted model counting and so we do not compare against them. Of particular note here is \tool{countAntom} \cite{BSB15}, a multi-core unweighted model counter. An interesting direction for future work is to explore the integration of weights into \tool{countAntom} and compare with tensor-network-based approaches to weighted model counting.

Existing approaches to weighted model counting can be split broadly into three categories: \emph{direct reasoning}, \emph{knowledge compilation}, and \emph{dynamic programming}. In counters based on direct reasoning (e.g., \tool{cachet} \cite{SBK05}), the idea is to reason directly about the CNF representation of $\varphi$. In counters based on knowledge compilation (e.g. \tool{miniC2D} \cite{OD15} and \tool{d4} \cite{LM17}), the idea is to compile $\varphi$ into an alternative representation on which counting is easy. In counters based on dynamic programming (e.g. \tool{ADDMC} \cite{DPV20} and \tool{gpuSAT2} \cite{FHWZ18,FHZ19}), the idea is to traverse the clause structure of $\varphi$. Tensor-network approaches to counting (e.g. \tool{TensorOrder} \cite{DDV19} and this work) are also based on dynamic programming. Dynamic programming approaches often utilize graph decompositions, which we define in the next section. 

% One example is \tool{cachet} \cite{SBK05}, which uses DPLL search combined with component caching and clause learning to perform weighted model counting. 
% Dynamic programming techniques are closely related to fixed-parameter algorithms for counting \cite{FMR08,SS10}. In particular, tensor network approaches can be seen as fixed-parameter algorithms for counting parameterized by the treewidth of the incidence graph (which is the bipartite graph where both variables and clauses are vertices and edges indicate that the variable appears in the connected clause). We define graph decompositions in the following section.

\subsection{Graphs and Graph Decompositions}
A \emph{graph} $G$ has a nonempty set of vertices $\V{G}$, a set of (undirected) edges $\E{G}$, a function $\delta_G: \V{G} \rightarrow 2^{\E{G}}$ that gives the set of edges incident to each vertex, and a function $\epsilon_G: \E{G} \rightarrow 2^{\V{G}}$ that gives the set of vertices incident to each edge. . Each edge must be incident to exactly two vertices, but multiple edges can
exist between two vertices. If $E \subset \E{G}$, let $\eincs{G}{E} = \bigcup_{e \in E} \einc{G}{e}$. % Similarly, if $V \subset \V{G}$, let $\vincs{G}{V} = \bigcup_{v \in V} \vinc{G}{v}$.

A \emph{tree} is a simple, connected, and acyclic graph. A \emph{leaf} of a tree $T$ is a vertex of degree one, and we use $\Lv{T}$ to denote the set of leaves of $T$. For every edge $a$ of $T$, deleting $a$ from $T$ yields exactly two trees, whose leaves define a partition of $\Lv{T}$. Let $C_a \subseteq \Lv{T}$ denote an arbitrary element of this partition. A \emph{rooted binary tree} is a tree $T$ where either $|\V{T}| = 1$ or every vertex of $T$ has degree one or three except a single vertex of degree two (called the \emph{root}). If $|\V{T}| > 1$, the \emph{immediate subtrees of $T$} are the two rooted binary trees that are the connected components of $T$ after the root is removed.

In this work, we use three decompositions of a graph as a tree: tree decompositions \cite{RS91}, branch decompositions \cite{RS91}, and carving decompositions \cite{ST94}. All decompose the graph into an \emph{unrooted binary tree}, which is a tree where every vertex has degree one or three. First, we define tree decompositions \cite{RS91}:
\begin{definition} %[Tree Decomposition]
	A \emph{tree decomposition} for a graph $G$ is an unrooted binary tree $T$ together with a labeling function $\chi : \V{T} \rightarrow 2^{\V{G}}$ such that: 
	% (1) $\forall v \in \V{G}$, $\exists n \in \V{T}$ s.t. $v \in \chi(n)$,
	(1) $\bigcup_{n \in \V{T}} \chi(n) = \V{G}$,
	(2) for all $e \in \E{G}$, there exists $n \in \V{T}$ s.t. $\einc{G}{e} \subseteq \chi(n)$, and 
	(3) for all $n, o, p \in \V{T}$, if $p$ is on the path from $n$ to $o$ then $\chi(n) \cap \chi(o) \subseteq \chi(p)$.
	
	%\begin{enumerate}
	%	\item For all $v \in \V{G}$, there exists $n \in \V{T}$ s.t. $v \in \chi(n)$.
	%	\item For all $e \in \E{G}$, there exists $n \in \V{T}$ s.t. $\einc{G}{e} \subseteq \chi(n)$.
	%	\item For all $n, o, p \in \V{T}$, if $p$ is on the path from $n$ to $o$ then $\chi(n) \cap \chi(o) \subseteq \chi(p)$.
	%\end{enumerate}
	
	The \emph{width} of a tree decomposition is $width_t(T, \chi) \equiv \max_{n \in \V{T}} | \chi(n) | - 1.$
	%, denoted $width_t(T, \chi)$, is the maximum size (minus 1) of the label of every vertex, i.e.,
	%$$width_t(T, \chi) = \max_{n \in \V{T}} | \chi(n) | - 1.$$
\end{definition}
The treewidth of a graph $G$ is the lowest width among all tree decompositions. Next, we define branch decompositions \cite{RS91}:
\begin{definition}%[Branch Decomposition]
\label{def:branch}
	A \emph{branch decomposition} for a graph $G$ with $\E{G} \neq \emptyset$ is an unrooted binary tree $T$ whose leaves are the edges of $G$, i.e. $\Lv{T} = \E{G}$. 
	
    The \emph{width} of $T$, denoted $width_b(T)$, is the maximum number of vertices in $G$ that are endpoints of edges in both $C_a$ and $\E{G} \setminus C_a$ for all $a \in \E{T}$, i.e.,
	$width_b(T) \equiv \max_{a \in \E{T}} \left| \eincs{G}{C_a} \cap \eincs{G}{\E{G} \setminus C_a} \right|.$
\end{definition}

The branchwidth of a graph $G$ is the lowest width among all branch decompositions. Carving decompositions are the dual of branch decompositions and hence can be defined by swapping the role of $\V{G}$ and $\E{G}$ in Definition \ref{def:branch}.

The treewidth (plus 1) of graph is no smaller than the branchwidth and is bounded from above by $3/2$ times the branchwidth \cite{RS91}. 

Given a CNF formula $\varphi$, a variety of associated graphs have been considered. The \emph{incidence graph} of $\varphi$ is the bipartite graph where both variables and clauses are vertices and edges indicate that the variable appears in the connected clause. The \emph{primal graph} of $\varphi$ is the graph where variables are vertices and edges indicate that two variables appear together in a clause. There are fixed-parameter tractable model counting algorithms with respect to the treewidth of the incidence graph and the primal graph \cite{SS10}. If the treewidth of the primal graph of a formula $\varphi$ is $k$, the treewidth of the incidence graph of $\varphi$ is at most $k+1$ \cite{KV00}.
 
%Next, we describe carving decompositions \cite{ST94}:
%\begin{definition}%[Carving Decomposition]
%\label{def:carving}
%	A \emph{carving decomposition} for a graph $G$ is an unrooted binary tree $T$ whose leaves are the vertices of $G$, i.e. $\Lv{T} = \V{G}$. 
%    The \emph{width} of $T$, denoted $width_c(T)$, is the maximum number of edges in $G$ between $C_a$ and $\V{G} \setminus C_a$ for all $a \in \E{T}$, i.e.,
%	$$width_c(T) = \max_{a \in \E{T}} \left| \vincs{G}{C_a} \cap \vincs{G}{\V{G} \setminus C_a} \right|.$$
%    The width of a carving decomposition $T$ with no edges is 0.
%\end{definition}

% theby thrice the carving width \cite{sasak10} and by . The branchwidth is no larger than the treewidth (plus 1) \cite{RS91}.

\subsection{Tensors, Tensor Networks, and Tensor-Network Contraction}
\emph{Tensors} are a generalization of vectors and matrices to higher dimensions-- a tensor with $r$ dimensions is a table of values each labeled by $r$ indices. 

Fix a set $\Ind$ and define an \emph{index} to be an element of $\Ind$. For each index $i$ fix a finite set $\domain{i}$ called the \emph{domain} of $i$. An index is analogous to a variable in constraint satisfaction. %For the algorithms in this work it is sufficient for every index to have domain $\{0, 1\}$.
An \emph{assignment} to $I \subseteq \Ind$ is a function $\tau$ that maps each index $i \in I$ to an element of $\domain{i}$. Let $\domain{I}$ denote the set of assignments to $I$, i.e., $\domain{I} = \{\tau: I \rightarrow \bigcup_{i \in I} \domain{i}~\text{s.t.}~\tau(i) \in \domain{i}~\text{for all}~i \in I\}.$

% Formally, fix a finite set $\Ind$ and finite sets $\domain{i}$ for each $i \in \Ind$. An \emph{index} is an element $i$ of $\Ind$, and the corresponding set $\domain{i}$ is called the \emph{domain} of $i$. 
%Fix a finite set $\Ind$, whose elements we call \emph{indices}, and for each index fix a finite set $\domain{i}$ called the \emph{domain} of $i$. 

We now formally define tensors as multidimensional arrays of values, indexed by assignments:
\begin{definition}[Tensor] \label{def:tensor}
	A \emph{tensor} $A$ over a finite set of indices (denoted $\tdim{A}$) is a function $A: \domain{\tdim{A}} \rightarrow \mathbb{R}$.
\end{definition}

The \emph{rank} of a tensor $A$ is the cardinality of $\tdim{A}$. The memory to store a tensor (in a dense way) is exponential in the rank. For example, a scalar is a rank 0 tensor, a vector is a rank 1 tensor, and a matrix is a rank 2 tensor. Some other works generalize Definition \ref{def:tensor} by replacing $\mathbb{R}$ with an arbitrary semiring. % (or, often, $\mathbb{C}$), but to ease exposition we do not.

A \emph{tensor network} defines a complex tensor by combining a set of simpler tensors in a principled way. This is analogous to how a database query defines a resulting table in terms of a computation across many tables.

\begin{definition}[Tensor Network]
	\label{def:tensor-contraction-network}
	A \emph{tensor network} $N$ is a nonempty set of tensors across which no index appears more than twice.
\end{definition}
The set of indices of $N$ that appear once (called \emph{free indices}) is denoted by $\tnfree{N}$. The set of indices of $N$ that appear twice (called \emph{bond indices}) is denoted by $\tnbound{N}$. 
% \emph{Free indices} of $N$ are indices that appear once, while \emph{bond indices} of $N$ are indices that appear twice. We denote the set of free indices of $N$ by $\tnfree{N}$ and the set of bond indices of $N$ by $\tnbound{N}$. 
% 
% The \emph{bond dimension} of $N$ is the maximum size of $\domain{i}$ for all bond indices $i$ of $N$.

% \emph{Free indices} of $N$ are indices that appear once, while \emph{bond indices} of $N$ are indices that appear twice. We denote the set of free indices of $N$ by $\tnfree{N}$ and the set of bond indices of $N$ by $\tnbound{N}$. % The \emph{bond dimension} of $N$ is the maximum size of $\domain{i}$ for all bond indices $i$ of $N$.

The problem of \emph{tensor-network contraction}, given an input tensor network $N$, is to compute the \emph{contraction} of $N$ by marginalizing all bond indices:
\begin{definition}[Tensor-Network Contraction]
The \emph{contraction} of a tensor network $N$ is a tensor $\tntensor{N}$ with indices $\tnfree{N}$ (the set of free indices of $N$), i.e. a function $\tntensor{N} : \domain{\tnfree{N}} \rightarrow \mathbb{R}$, that is defined for all $\tau \in \domain{\tnfree{N}}$ by
		\begin{equation}
        \label{eqn:contraction} 
        \tntensor{N}(\tau) \equiv \sum_{\rho \in \domain{\tnbound{N}}} \prod_{A \in N} A((\rho \cup \tau)\restrict{\tdim{A}}).
        \end{equation}
\end{definition}

A tensor network $N'$ is a \emph{partial contraction} of a tensor network $N$ if there is a surjective function $f: N \rightarrow N'$ s.t. for every $A \in N'$ we have $\tntensor{f^{-1}(A)} = A$; that is, if every tensor in $N'$ is the contraction of some tensors of $N$. If $N'$ is a partial contraction of $N$, then $\tntensor{N'} = \tntensor{N}$.

 Let $A$ and $B$ be tensors. Their \emph{contraction} $A \cdot B$ is the contraction of the tensor network $\{A, B\}$. If $\tdim{A} = \tdim{B}$, their \emph{sum} $A+B$ is the tensor with indices $\tdim{A}$ whose entries are given by the sum of the corresponding entries in $A$ and $B$. 


% Following our analogy, given a tensor network containing database tables (under bag-semantics) as tensors, its contraction is the join of those tables followed by the projection of all shared attributes. Thus a tensor network is analogous to a project-join query.
A tensor network can also be seen as a variant of a factor graph \cite{KFL01} with the additional restriction that no variable appears more than twice. The contraction of a tensor network corresponds to the marginalization of a factor graph \cite{RS17}, which is a a special case of the sum-of-products problem \cite{BDP09,dechter99} and the FAQ problem \cite{KNR16}. The restriction on variable appearance is heavily exploited in tools for tensor-network contraction and in this work, since it allows tensor contraction to be implemented as matrix multiplication and so leverage significant work in high-performance computing on matrix multiplication on CPUs \cite{LHKK77} and GPUs \cite{FSH04}.