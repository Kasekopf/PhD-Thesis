\section{Implementation and Evaluation}
\label{sec:experiments}

We aim to answer the following experimental research questions:
\begin{enumerate}\itemsep0em 
    \item[(RQ1)] Is the planning phase improved by a parallel portfolio of decomposition tools?
    
    \item[(RQ2)] Is the planning phase improved by adding a branch-decomposition tool?
    
    \item[(RQ3)] When should Algorithm \ref{alg:wmc} transition from the planning phase to the execution phase (i.e., what should be the value of the performance factor $\alpha$)?
    
    \item[(RQ4)] Is the execution phase improved by leveraging multiple cores and a GPU?
    
    \item[(RQ5)] Is the execution phase improved by leveraging a TPU in graph execution mode?
    
    \item[(RQ6)] Do parallel tensor network approaches improve a portfolio of state-of-the-art weighted model counters (\tool{cachet}, \tool{miniC2D}, \tool{d4}, \tool{ADDMC}, and \tool{gpuSAT2})?
\end{enumerate}

We implement our changes on top of \tool{TensorOrder} (which implements Algorithm \ref{alg:wmc}; see Section \ref{sec:tensors:experiments:implementation}) to produce \tool{TensorOrder2}, a new parallel weighted model counter. Implementation details are described in Section \ref{sec:experiments:impl}. All code is available at  \url{https://github.com/vardigroup/TensorOrder}.

%\paragraph{Benchmarks.} 
We use a standard set\footnote{\url{https://github.com/vardigroup/ADDMC/releases/tag/v1.0.0}} of 1914 weighted model counting benchmarks \cite{DPV20}. Of these, 1091 benchmarks\footnote{\url{https://www.cs.rochester.edu/u/kautz/Cachet/}} are from Bayesian inference problems \cite{SBK05} and 823 benchmarks\footnote{\url{http://www.cril.univ-artois.fr/KC/benchmarks.html}} are unweighted benchmarks (from various domains) that were artificially weighted by \cite{DPV20}. For weighted model counters that cannot handle real weights larger than 1 (\tool{cachet} and \tool{gpusat2}), we rescale the weights of benchmarks with larger weights. In Experiment 3, we also consider preprocessing these 1914 benchmarks by applying $\pkg{pmc-eq}$ \cite{LM14} (which preserves weighted model count). % We apply to each benchmark, with a timeout of 1000 seconds. We remove 61 benchmarks that were fully solved by $\pkg{pmc-eq}$ (either UNSAT., or with a single solution) and 11 benchmarks that timed out, resulting in 1842 preprocessed benchmarks.
We evaluate the performance of each tool using the PAR-2 score, which is the sum of of the wall-clock times for each completed benchmark, plus twice the timeout for each uncompleted benchmark.

%\paragraph{Experimental Setup.}
All counters are run in Docker images (one for each counter) with Docker 19.03.5. All experiments are run on Google Cloud \tool{n1-standard-8} machines with 8 cores (Intel Haswell, 2.3 GHz) and 30 GB RAM. GPU-based counters are provided an \tool{NVIDIA Tesla V100} GPU (16 GB of onboard RAM) using NVIDIA driver 418.67 and CUDA 10.1.243. TPU-based counters are provided a v2-8 TPUs, which contains 8 TPU cores and 8 GB of onboard RAM per core.

\subsection{Implementation Details of \tool{TensorOrder2}}
\label{sec:experiments:impl}
\tool{TensorOrder2} is primarily implemented in Python 3 as a modified version of the tool \tool{TensorOrder} from Chapter \ref{ch:tensors}. We replace portions of the Python code with C++ (\tool{g++} v7.4.0) using Cython 0.29.15 for general speedup, especially in the implementation of the \textbf{FT} planner.

\paragraph{Planning.} 
\tool{TensorOrder} contains an implementation of the planning phase using the \textbf{FT} planner together with a choice of three single-core tree-decomposition solvers: \pkg{Tamaki} \cite{Tamaki17}, \pkg{FlowCutter} \cite{HS18}, and \pkg{htd} \cite{AMW17}. We add to \tool{TensorOrder2} an implementation of Theorem \ref{thm:factorable-branch} and use it to add a branch-decomposition solver \pkg{Hicks} \cite{hicks02}.
We implement a parallel portfolio of graph-decomposition solvers in C++ and give \tool{TensorOrder2} access to two portfolios, each with access to all cores: \pkg{P3} (which combines \pkg{Tamaki},  \pkg{FlowCutter}, and \pkg{htd}) and \pkg{P4} (which includes \pkg{Hicks} as well).

\paragraph{Execution.} 
\tool{TensorOrder} is able to perform the execution phase on a single core and on multiple cores using \pkg{numpy} v1.18.1 and \pkg{OpenBLAS} v0.2.20. 

We add to \tool{TensorOrder2} a way to contract tensors on a GPU with \pkg{TensorFlow} v2.1.0 \cite{ABCCDDDGII16}. To avoid GPU kernel calls for small contractions, \tool{TensorOrder2} uses a GPU only for contractions where one of the tensors involved has rank $\geq 20$, and reverts back to using multi-core \pkg{numpy} otherwise. We also add an implementation of Algorithm \ref{alg:tn-sliced}.

We also add a way to contract tensors on a TPU with \pkg{jax} v0.2.13 \cite{jax2018github} in graph execution mode. 
We execute each slice (i.e., each call to \func{Execute} on Line \ref{line:tn-sliced:execute} of Algorithm \ref{alg:tn-sliced}) on a separate TPU core. 
Thus on v2-8 TPUs we execute 8 slices in parallel. 
For comparison, we also add support for contracting tensors on multiple-CPUs with \pkg{jax} in graph execution mode.

Overall, \tool{TensorOrder2} runs the execution phase in three possible configurations in eager execution mode-- \pkg{CPU1} (restricted to a single CPU core), \pkg{CPU8} (allowed to use all 8 CPU cores), \pkg{GPU} (allowed to use all 8 CPU cores and use a GPU)-- and two possible configurations in graph execution mode-- \pkg{TPU-graph} (allowed to use all 8 CPU cores, and a TPU), and \pkg{CPU8-graph} (allowed to use all 8 CPU cores).

Finally, note that \tool{TensorOrder2} supports 64-bit floats for all execution configurations except for \pkg{TPU-graph}, which supports only 32-bit floats due to TPU hardware limitations. We therefore output 64-bit floats in Experiments 1, 2, and 3, and output 32-bit floats for Experiment 4.

\begin{figure}
	\centering
	\input{figures/5/planning.pgf}
    \vspace*{-0.9cm}
	\caption{\label{fig:parallel:planning} A cactus plot of the performance of various planners. A planner ``solves'' a benchmark when it finds a contraction tree of max rank 30 or smaller.}
\end{figure}

\subsection{Experiment 1: The Planning Phase (RQ1 and RQ2)}
We run each planning implementation (\pkg{FlowCutter}, \pkg{htd}, \pkg{Tamaki}, \pkg{Hicks}, \pkg{P3}, and \pkg{P4}) once on each of our 1914 benchmarks and save all contraction trees found within 1000 seconds (without executing the contractions). Results are summarized in Figure \ref{fig:parallel:planning}. 

% In this figure, we consider a benchmark to be solved by a planning implementation when the planner is able to find contraction tree whose max-rank is 30 or smaller.

%\begin{figure}[t]
%\begin{center}
%\input{figures/planning.pgf}
%\includegraphics[height=2in,width=2.5in]{figures/planning.pdf}
%\caption{\label{fig:parallel:planning} A cactus plot of the performance of various planners. A planner ``solves'' a benchmark when it finds a contraction tree of max rank 30 or smaller.}
%\end{center}
%\end{figure}


We observe that the parallel portfolio planners outperform all four single-core planners after 5 seconds. In fact, after 20 seconds both portfolios perform almost as well as the virtual best solver. We conclude that portfolio solvers significantly speed up the planning phase.

We also observe that \pkg{P3} and \pkg{P4} perform almost identically in Figure \ref{fig:parallel:planning}. Although after 1000 seconds \pkg{P4} has found better contraction trees than \pkg{P3} on 407 benchmarks, most improvements are small (reducing the max-rank by 1 or 2) or still do not result in good-enough contraction trees. We conclude that adding \pkg{Hicks} improves the portfolio slightly, but not significantly.
 
\subsection{Experiment 2: Determining the Performance Factor (RQ3)}
\label{sec:experiments:pf}
We take each contraction tree discovered in Experiment 1 (with max-rank below 36) and use \tool{TensorOrder2} to execute the tree with a timeout of 1000 seconds on each of the three hardware configurations in eager execution mode (\pkg{CPU1}, \pkg{CPU8}, and \pkg{GPU}). We observe that the max-rank of almost all solved contraction trees is 30 or smaller.

Given a performance factor, a benchmark, and a planner, we use the planning times from Experiment 1 to determine which contraction tree would have been chosen in step 4 of Algorithm \ref{alg:wmc}. We then add the execution time of the relevant contraction tree on each hardware. In this way, we simulate Algorithm \ref{alg:wmc} for a given planner and hardware with many performance factors. 

\begin{figure}[t]
\begin{center}
\input{figures/5/performance_factor.pgf}
\vspace*{-0.9cm}
\caption{\label{fig:performance-factor} A graph of the simulated PAR-2 score for various combinations of planners and hardware as the performance factor varies.}
\end{center}
\end{figure}

\begin{table}[t]
  \caption{\label{tab:performance_factor} The performance factor for each combination of planner and hardware that minimizes the simulated PAR-2 score.}
  \vspace*{0.1cm}
  \centering
    \begin{tabular}{l|c|c|c|c|c|c|}
 & \pkg{Tamaki} & \pkg{FlowCutter} & \pkg{htd} & \pkg{Hicks} & \pkg{P3} & \pkg{P4}\\ \hline 
\pkg{CPU1} & $3.8\cdot 10^{-11}$ & $4.8\cdot 10^{-12}$ & $1.6\cdot 10^{-12}$ & $1.0\cdot 10^{-21}$ & $1.4\cdot 10^{-11}$ & $1.6\cdot 10^{-11}$\\ \hline 
\pkg{CPU8} & $7.8\cdot 10^{-12}$ & $1.8\cdot 10^{-12}$ & $1.3\cdot 10^{-12}$ & $1.0\cdot 10^{-21}$ & $5.5\cdot 10^{-12}$ & $6.2\cdot 10^{-12}$\\ \hline 
\pkg{GPU} & $2.1\cdot 10^{-12}$ & $5.5\cdot 10^{-13}$ & $1.3\cdot 10^{-12}$ & $1.0\cdot 10^{-21}$ & $3.0 \cdot 10^{-12}$ & $3.8\cdot 10^{-12}$\\ \hline 
    \end{tabular}
\end{table}

Figure \ref{fig:performance-factor} indicates how varying the performance factor affects the simulated PAR-2 score for various combinations of planners and hardware. For each planner and hardware, Table 2 shows the performance factor $\alpha$ that minimizes the simulated PAR-2 score. We observe that the performance factor for \pkg{CPU8} is lower than for \pkg{CPU1}, but not necessarily higher or lower than for \pkg{GPU}. We conclude that different combinations of planners and hardware are optimized by different performance factors. % Further details are available in the supplemental material.

% Selected results are summarized in Figure \ref{fig:performance-factor}. For each planner and hardware, we select the performance factor $\alpha$ that minimizes the simulated PAR-2 score (i.e., the sum of of the wall-clock times for each completed benchmark, plus 2000 for each uncompleted benchmark). We observe that the performance factor for $\pkg{GPU}$ is typically lower than for $\pkg{CPU8}$, which is lower than for $\pkg{CPU1}$. We conclude that different combinations of planners and hardware are optimized by different performance factors. The full set of selected values is available in the supplemental material.

\begin{figure}[tp]
\begin{center}
\input{figures/5/comparison_all.pgf}
\vspace*{-0.9cm}
\caption{\label{fig:parallel:comparison} A cactus plot of the number of benchmarks solved by various counters, without (above) and with (below) the \tool{pmc-eq}  preprocessor.}
\end{center}
\vspace*{-0.8cm}
\end{figure}

\begin{table}[t]
  \caption{\label{tab:comparison} The numbers of benchmarks solved by each counter fastest and in total after 1000 seconds, and the PAR-2 score.}
  \vspace*{0.1cm}
  \centering
  \begin{tabular}{l||r|r|r||r|r|r|}
  & \multicolumn{3}{c||}{Without preprocessing} & \multicolumn{3}{c|}{With \tool{pmc-eq} preprocessing} \\
  & \# Fastest & \# Solved & PAR-2 & \# Fastest & \# Solved & PAR-2\\ \hline 
\pkg{T.}+\pkg{CPU1} & 0 & 1151 & 1640803. & 0 & 1453 & 811199.\\ \hline 
\pkg{P4}+\pkg{CPU1} & 43 & 1164 & 1562474. & 80 & 1465 & 782475.\\ \hline 
\pkg{P4}+\pkg{CPU8} & 48 & 1185 & 1500968. & 58 & 1481 & 748745.\\ \hline 
\pkg{P4}+\pkg{GPU} & 60 & 1210 & 1436949. & 38 & 1488 & 722582.\\ \hline 
\tool{miniC2D} & 40 & 1381 & 1131457. & 124 & 1582 & 562259.\\ \hline 
\tool{d4} & 541 & 1508 & 883829. & 436 & 1514 & 723207.\\ \hline 
\tool{cachet} & 230 & 1363 & 1156309. & 207 & 1330 & 1075215.\\ \hline 
\tool{ADDMC} & 184 & 1302 & 1245470. & 33 & 1290 & 1147473.\\ \hline 
\tool{gpusat2} & 36 & 1258 & 1342646. & 24 & 1436 & 831762.\\ \hline 
\tool{DPMC} & 585 & 1279 & 1301363. & 654 & 1427 & 849168.\\ \hline 
\end{tabular}
\end{table}

\subsection{Experiment 3: End-to-End Performance (RQ4 and RQ6)}
Next, we compare \tool{TensorOrder2} with existing state-of-the-art weighted model counters \tool{cachet}, \tool{miniC2D}, \tool{d4}, \tool{ADDMC}, and \tool{gpuSAT2}. We consider \tool{TensorOrder2} using \pkg{P4} as the planner combined with each the three hardware configurations in eager execution mode (\pkg{CPU1}, \pkg{CPU8}, and \pkg{GPU}), along with \pkg{Tamaki}+\pkg{CPU1} as the best non-parallel configuration from \cite{DDV19}. Note that \tool{P4}+\tool{CPU1} still leverages multiple cores in the planning phase. The performance factor from Experiment 2 is used for each \tool{TensorOrder2} configuration.

We run each counter once on each benchmark (both with and without \pkg{pmc-eq} preprocessing) with a timeout of 1000 seconds and record the wall-clock time taken. When preprocessing is used, both the timeout and the recorded time include preprocessing time. For \tool{TensorOrder2}, recorded times include all of Algorithm \ref{alg:wmc}. Results are summarized in Figure \ref{fig:parallel:comparison} and Table \ref{tab:comparison}. 

We observe that \tool{TensorOrder2} is improved by the portfolio planner and, on hard benchmarks, by executing on a multi-core CPU and on a GPU. The flat line at 3 seconds for $\pkg{P4}+\pkg{GPU}$ is caused by overhead from initializing the GPU.

Comparing \tool{TensorOrder2} with the other counters, \tool{TensorOrder2} is competitive without preprocessing but solves fewer benchmarks than all other counters, although \tool{TensorOrder2} (with some configuration) is faster than all other counters on 158 benchmarks before preprocessing. 
We observe that preprocessing significantly boosts \tool{TensorOrder2} relative to other counters, similar to prior observations with \tool{gpusat2} \cite{FHZ19}. \tool{TensorOrder2} solves the third-most preprocessed benchmarks of any solver and has the second-lowest PAR-2 score (notably, outperforming \tool{gpusat2} in both measures). \tool{TensorOrder2} (with some configuration) is faster than all other counters on 176 benchmarks with preprocessing. Since \tool{TensorOrder2} improves the virtual best solver on 151 benchmarks without preprocessing and on 176 benchmarks with preprocessing, we conclude that \tool{TensorOrder2} is useful as part of a portfolio of counters.

\subsection{Experiment 4: Executing on a TPU (RQ5)}
\label{sec:parallel:exp:tpu}
\begin{figure}[t]
\begin{center}
\input{figures/5/tpu.pgf}
%\input{figures/comparison_pmc_eq.pgf}
\vspace*{-0.9cm}
\caption{\label{fig:parallel:tpu} The compilation and average execution time of contracting a single tensor-network slice with a contraction tree of max-rank $k$ on a TPU (\tool{TPU-graph}), on a CPU in graph-execution mode (\tool{CPU8-graph}), and on a CPU in eager execution mode (\tool{CPU8}). \tool{TPU-graph} took more than 1000 seconds in the compilation step when $k > 17$.}
\end{center}
\vspace*{-0.8cm}
\end{figure}

Finally, we examine the feasibility of leveraging a TPU in the execution phase. We first run the \pkg{TPU-graph} executor manually on a subset of benchmarks from Experiment 3. Unfortunately, we were unable to find nontrivial benchmarks (i.e., benchmarks that took more than 1 second to solve in \tool{TensorOrder2} using a single CPU core) that were solvable by \pkg{TPU-graph} within 1000 seconds.

% Based on Section \ref{sec:parallel:execution:tpu}, we expect TPU approaches to perform well on benchmarks that require significant slicing. 

% We first consider a set of 32 tensor network benchmarks $N$ where every counter in Experiment 3 timed out but where $\tool{P4}$ was able to find a contraction tree $T$ for $N$ and a set of slice variables $I \subseteq \tnbound{N}$ (with a timeout of 1000 seconds) s.t. $|I| < 20$ and the memory cost $\func{MemCost}(N,T,I)$ is below 8GB (the memory available on each TPU core).
% On each benchmark $(N, T, I)$ with a timeout of 1000 seconds, we run $\func{Execute}(N[\eta], T[\eta])$ using \pkg{TPU-graph} for each $\eta \in [I]$ and record the compilation time along with the total execution time.
% Unfortunately, we observe that on all 32 benchmarks the compilation takes more than 1000 seconds and so the computation times out.

To investigate this failure, we consider a tensor network $N$ whose contraction is the number of vertex covers of a randomly-generated cubic graph with 200 vertices \cite{KCMR18}. 
Using the \pkg{FlowCutter} planner, we construct a contraction tree $T$ for $N$ of max-rank 27. 
For each $k \in \{10, 11, \cdots, 20\}$, then, we slice $N$ greedily to get slice variables $I_k \subseteq \tnbound{N}$ so that $T[I_k]$ has max-rank $k$. 
Using each of the execution configurations \pkg{CPU8}, \pkg{CPU8-graph}, and \pkg{TPU-graph}, we compute $\func{Execute}(N[\eta], T[\eta])$ for the first 80 slices $\eta \in [I_k]$. 
We then compute the average time to contract each slice and (for configurations in graph execution mode) the XLA compilation time.

Results are summarized in Figure \ref{fig:parallel:tpu}. 
We observe that, as expected, the compilation time in graph execution mode (both \pkg{CPU8-graph} and \pkg{TPU-graph}) is significantly longer than the time for a single slice in eager execution mode (\pkg{CPU8}). 
Moreover, the execution time in graph execution mode is faster than the execution time in eager execution mode.

We also observe that the compilation time of \pkg{TPU-graph} scales dramatically with sliced max-rank, which is the maximum number of indices of the tensors involved in the computation.
On tensors with more than 17 indices the compilation time of \pkg{TPU-graph} is above 1000 seconds and so the compilation times out.
For comparison, the max-rank of nontrivial benchmarks in Experiment 3 is always more than 25, even after significant slicing.
On the other hand, \pkg{CPU8-graph} does not suffer from long XLA compilation times even on high-dimensional tensors and so is promising for future analysis.

Finally, we observe that the execution time of \pkg{CPU8-graph} is less than the execution time of \pkg{TPU-graph}.
We hypothesize that this is an artifact of the small tensors involved in these experiments. Since every index in the tensors we consider has size 2, tensors with no more than 17 indices are no larger than 1MB.
Once the XLA compilation bottleneck is improved and tensor networks with larger tensors may be used, the execution time of \pkg{TPU-graph} may outperform \pkg{CPU8-graph}.

We conclude that \pkg{TPU-graph} is currently unsuitable for nontrivial model counting benchmarks because of long XLA compilation times for high-dimensional tensors, i.e. tensors with more than 17 indices.
We hypothesize that XLA compilation times are long for high-dimensional tensors because of optimizations for neural network training and inference. 
Unlike our setting, where tensors often have many indices (more than 25) but each index has a domain of size 2, tensors in neural network training and inference typically have relatively few indices (less than 5) but each index has a much larger domain (thousands). %TODO: cite?

We emphasize that these results should be taken only as initial observations.
% We hope that future development of tensor libraries for TPUs, especially in the compilation of the XLA graph programs with high-dimensional tensors, will improve performance.
Further analysis of XLA compilation with high-dimensional tensors may help to understand and improve the poor performance of tensor-network-based methods for model counting on a TPU.
Unfortunately, the implementation of the XLA compiler for a TPU is proprietary and available for use only as a black-box tool. 
We thus leave an in-depth analysis for future work.
