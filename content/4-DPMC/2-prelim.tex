\section{Preliminaries}
\label{sec_prelim}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{\textbf{Pseudo-Boolean Functions and Early Projection}}

A \emph{pseudo-Boolean function} over a set $X$ of variables is a function $f: 2^X \to \R$.
Operations on pseudo-Boolean functions include \emph{product} and \emph{projection}.
First, we define product.
\begin{definition}[Product]
\label{def_mult}
    Let $X$ and $Y$ be sets of Boolean variables.
    The \textdef{product} of functions $f: 2^X \to \R$ and $g: 2^Y \to \R$ is the function $f \mult g: 2^{X \cup Y} \to \R$ defined for all $\tau \in 2^{X \cup Y}$ by
    $(f \mult g)(\tau) \equiv f(\tau \cap X) \mult g(\tau \cap Y).$
\end{definition}

Next, we define (additive) projection, which marginalizes a single variable.
\begin{definition}[Projection]
\label{def_proj}
    Let $X$ be a set of Boolean variables and $x \in X$.
    The \textdef{projection} of a function $f: 2^X \to \R$ \wrt{} $x$ is the function $\proj_x f: 2^{X \setminus \set{x}} \to \R$ defined for all $\tau \in 2^{X \setminus \set{x}}$ by
    $\pars{\proj_x f}(\tau) \equiv f(\tau) + f(\tau \cup \set{x}).$
\end{definition}
Note that projection is commutative, \ie, that $\proj_x \proj_y f = \proj_y \proj_x f$ for all variables $x, y \in X$ and functions $f: 2^X \to \R$.
Given a set $X = \set{x_1, x_2, \ldots, x_n}$, define
$\proj_X f \equiv \proj_{x_1} \proj_{x_2} \ldots \proj_{x_n} f$.
Our convention is that $\proj_\emptyset f \equiv f$.

When performing a product followed by a projection, it is sometimes possible to perform the projection first.
This is known as \emph{early projection} \cite{MPPV04}.
\begin{theorem}[Early Projection]
\label{thm_early_proj}
    Let $X$ and $Y$ be sets of variables.
    For all functions $f: 2^X \to \R$ and $g: 2^Y \to \R$, if $x \in X \setminus Y$, then $\proj_x (f \mult g) = \pars{\proj_x f} \mult g.$
    % As a corollary, for all $X' \subseteq X \setminus Y$,
    % $$\exists_{X'} (A \mult B) = \pars{\exists_{X'} A} \mult B.$$
\end{theorem}
Early projection is a key technique in symbolic computation in a variety of settings, including database-query optimization \cite{KV00}, symbolic model checking \cite{burch1991symbolic}, satisfiability solving \cite{pan2005symbolic}, and model counting \cite{DPV20}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{\textbf{Weighted Model Counting}}

The weighted model count of $\phi$ \wrt{} $W$ can be naturally expressed in terms of pseudo-Boolean functions: $W(\phi) = \left(\sum_X (\phi \mult W) \right)(\emptyset)$.
The function $W: 2^X \to \R$ is called a \textdef{weight function}.
In this work, we focus on {literal-weight functions}, which can be expressed as products of weights associated with each variable.
Formally, a \textdef{literal-weight function} $W$ can be factored as $W = \prod_{x \in X} W_x$ for pseudo-Boolean functions $W_x: 2^{\{x\}} \to \R$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \paragraph\textbf{{Algebraic Decision Diagrams}}

% A \emph{Binary Decision Diagram}, or BDD, is a data structure that represents a Boolean function $f : \{0, 1\}^n \to \{0, 1\}$ as a directed acyclic graph \cite{bryant1986graph}.
% BDDs have previously been applied to satisfiability checking \cite{bryant1992symbolic,pan2004search} and constrained counting \cite{lobbing1996number,minato1993zero}, but these approaches fail to generalize to large problem instances.
% In particular, these approaches for constrained counting require constructing a monolithic (possibly exponentially-sized) BDD as an intermediate step and so quickly become infeasible in practice.
% An \emph{Algebraic Decision Diagram}, or ADD, is a generalization of a BDD that allows for non-Boolean values at leaf nodes \cite{bahar1997algebric}.
% An ADD represents a function $f : \{0, 1\}^n \to S$ for an arbitrary domain $S$.
% ADDs have been successfully applied to solve propositionally factored Markov Decision Processes \cite{hoey1999spudd} and in model checking \cite{clarke2018model,hansen2002symbolic}.
% There are libraries (\eg, \cite{somenzi2015cudd}) that efficiently implement a variety of operations on ADDs, including product and projection.

%% Vu: the following paragraph reveals our identities and is unsuitable for blind review
% Although Binary Decision Diagrams (BDDs) have been previously used for constrained counting \cite{lobbing1996number,minato1993zero}, these approaches require constructing a monolithic (possibly exponentially-sized) BDD representing the input formula $F$ as an intermediate step and so quickly become infeasible in practice.
% In recent work \cite{DPV20} We proposed to modify these approaches to construct an ADD representing the formula $F$, with the weight function giving the value of each leaf node of the ADD.
% To make this approach successful, we developed algebraic techniques of ``early quantification'', for ADDs, extending those developed for BDDs \cite{burch1991symbolic,tabajara2017factored}.
% The goal is to iteratively reduce the size of the ADD during the construction and therefore avoid constructing the entire ADD at once.
% We showed that this approach provides significant speedups over prior techniques that use BDDs for constrained counting.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \paragraph\textbf{{Tensors}}

% A tensor is a multi-dimensional generalization of a matrix.
% Tensor are widely used in data analysis \cite{Cichocki14}, signal and image processing \cite{cichocki2015tensor}, quantum physics \cite{arad2010quantum}, quantum chemistry \cite{smilde2005multi}, and many other areas of science.
% Given the diverse applications of tensors and tensor networks, a variety of tools \cite{baumgartner2005synthesis,KKCLA17} exist to manipulate them efficiently.

% Tensors can be used to represent pseudo-Boolean functions in a dense way.
% Tensors are particularly efficient at computing the \emph{contraction} of two pseudo-Boolean functions: given two functions $f: 2^X \to \R$ and $g: 2^Y \to \R$, their contraction is the pseudo-Boolean function $\proj_{X \cap Y} f \cdot g$.
% The contraction of two tensors can be implemented as matrix multiplication and so leverage significant work in high-performance computing on matrix multiplication on CPUs \cite{LHKK77} and GPUs \cite{FSH04}.

% A tensor network is a representation of a complex tensor by interconnected, simpler tensors.
% The problem of \emph{tensor-network contraction}, given a tensor network, is to compute explicitly the tensor represented by the network.
% For a more formal definition of tensors, tensor networks, and tensor-network contraction, see \cite{arad2010quantum}.
% Given the diverse applications of tensors and tensor networks, a variety of tools \cite{baumgartner2005synthesis,KKCLA17} exist to manipulate them efficiently.

% An algorithm to reduce constrained counting (with a constant weight function) to tensor-network contraction was proposed recently \cite{BMT15}.
% The analysis of this algorithm has so far been purely theoretical, focusing, particularly, on identifying classes of formulas $F$ for which the corresponding tensor-network contraction can be performed in polynomial time \cite{BMT15}.
% We have recently explored implementing this algorithm on top of state-of-the-art tools for tensor-network contraction \cite{baumgartner2005synthesis,KKCLA17} to obtain a tool for constrained counting \cite{DDV19}.

% Algorithms for tensor network contraction rely on finding a good order to contract the tensors of the tensor network.
% This problem is closely connected to join-query optimization in database theory \cite{wong1976decomposition,MPPV04}, but this connection was largely unexplored by prior work.
% The connection between Boolean satisfiability (and hence constrained counting) and join-query optimization has been explored previously \cite{DKV02}.
% By improving the connection between tensor-network contraction and constrained counting, we aimed to use results from join-query optimization (in particular, on using graph decompositions \cite{GLST17,MPPV04}) to improve tensor-network contraction both for constrained counting and for more general problems.
% We showed tensor networks are a significant novel addition to the portfolio of constrained-counting algorithms.
