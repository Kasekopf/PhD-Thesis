\subsection{Measuring Planner Quality}
\label{sec:tensors:planning:quality}
There are a variety of existing planning techniques that each aim to minimize different notions of computational cost of Algorithm \ref{alg:network-contraction}.

Firstly, there are several \emph{cost-based} approaches aim for minimizing the total number of floating point operations (specifically, in Line \ref{line:network-contraction:subcontraction} of Algorithm \ref{alg:network-contraction}).
Examples include \cite{PHV14}, the \pkg{einsum} package in \pkg{numpy}, and the Tensor Contraction Engine \cite{Hirata03}, which performs additional optimizations of Algorithm \ref{alg:network-contraction} at the compiler level. %TODO: Lookup PHV14
% Unfortunately, cost-based approaches often rely on a brute-force analysis of all possible contraction trees, or on heuristic perturbations of an existing contraction tree.
Such approaches typically focus on tensor networks with relatively few tensors (tens) but large indices (hundreds of thousands of elements in each domain).
Most cost-based approaches are therefore unsuitable for weighted model counting, where we encounter tensor networks with many tensors (thousands) but where each index has a domain of size 2.

Instead, we focus on \emph{structure-based} approaches to planning, which analyze the rank of intermediate tensors that appear during recursive calls of Algorithm \ref{alg:network-contraction}. 
These ranks indicate the amount of memory and computation required at each recursive stage. Moreover, these ranks are more amenable to analysis.

One line of work \cite{MS08,DFGHSW18} uses graph decompositions to analyze the \emph{contraction complexity} of a contraction tree: the maximum over all recursive calls of the sum (minus 1) of the rank of the two tensors contracted in Line \ref{line:network-contraction:subcontraction} of Algorithm \ref{alg:network-contraction}. In our factor graph analogy, contraction complexity of a contraction tree corresponds to the \emph{width} of a dtree (i.e., the size of the largest cluster) \cite{darwiche01b}.
Contraction complexity measures the memory required when Line \ref{line:network-contraction:subcontraction} is computed by summing over the shared indices separately, i.e., first contracting $A_1$ and $A_2$ as if they have no common indices (producing and storing a rank $|\tdim{A_1}|+|\tdim{A_2}|$ tensor) and then sequentially contracting each pair of shared indices. In this approach, the memory required is exponential in the contraction complexity. Modern tensor packages (e.g. \pkg{numpy}), however, instead sum over all shared indices simultaneously without storing an intermediate result. This alternative approach requires the same number of floating-point operations, but often requires significantly less intermediate memory. Thus contraction complexity overestimates the memory requirements of many contraction trees. 

Instead, another line of planning approaches analyzes the maximum rank over all recursive calls of the result of Line \ref{line:network-contraction:subcontraction} (and Line \ref{line:network-contraction:lookup}). We call this the \emph{max rank} of a contraction tree. In our factor graph analogy, max rank of a contraction tree corresponds to the size of the largest separator of a dtree \cite{darwiche01b} and is closely connected to the performance of the Shenoy-Shafer algorithm \cite{shenoy97,SS08}. Max rank measures the memory required when Line \ref{line:network-contraction:subcontraction} is computed by summing over all shared indices simultaneously without storing an intermediate result: the memory required is exponential in the max rank. Thus, max rank estimates the memory usage of modern tensor packages. While the max rank is no more than linearly smaller than the contraction complexity, both appear in the exponent of the required memory. Therefore even small differences between max rank and contraction complexity can cause very different memory usage. 

We therefore choose max rank as the most appropriate measure of contraction tree quality in this setting. 
Recent work \cite{KCMR18} introduced three planning techniques that heuristically minimize the max rank: a greedy approach (called \textbf{greedy}), an approach using graph partitioning (called \textbf{metis}), and an approach using community-structure detection (called \textbf{GN}). Note that \textbf{metis} is analogous to earlier work on using graph partitioning to construct dtrees with small separators \cite{darwiche01b}.