\section{Chapter Summary}
In this work, we explored the impact of multiple-core and GPU use on tensor network contraction for weighted model counting. We implemented our techniques in \tool{TensorOrder2}, a new parallel counter, and showed that \tool{TensorOrder2} is useful as part of a portfolio of counters.

In the planning stage, we showed that a parallel portfolio of graph-decomposition solvers is significantly faster than single-core approaches. We proved that branch decomposition solvers can also be included in this portfolio, but concluded that a state-of-the-art branch decomposition solver only slightly improves the portfolio. In the execution stage, we showed that tensor contractions can be performed with \pkg{TensorFlow} on a GPU. When combined with index slicing, we concluded that a GPU speeds up the execution stage for many hard benchmarks. For easier benchmarks, the overhead of a GPU may outweigh any contraction speedups. 
