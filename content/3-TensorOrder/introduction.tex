% Constrained counting can be reduced to the problem of tensor-network contraction \cite{BMT15}. \emph{Tensor networks} are a tool used across quantum physics and computer science for describing and reasoning about quantum systems, big-data processing, and more \cite{BB17,Cichocki14,Orus19}. A tensor network describes a complex tensor as a computation on many simpler tensors, and the problem of tensor-network \emph{contraction} is to perform this computation. Although tensor networks can be seen as a variant of factor graphs \cite{KFL01}, working directly with tensor networks allows us to leverage massive practical work in machine learning and high-performance computing on tensor contraction \cite{BK07,Hirata03,KKCLA17,VZTGDMVAC18} (which also includes GPU support \cite{KSTKPPRS19,NRBHHJN15}) to perform constrained counting. Since tensor networks are relatively unknown in the artificial intelligence community, we give an introduction of relevant material on tensors and tensor networks in Section \ref{sec:tensors:tensors}.

%TODO: Discuss reduction to weighted model counting

Contracting a tensor network requires determining an order to contract the tensors inside the network, and so efficient contraction requires finding a contraction order that minimizes computational cost. Since the number of possible contraction orders grows exponentially in the number of tensors, cost-based exhaustive algorithms, e.g. \cite{PHV14}, cannot scale to handle the large tensor networks required for the reduction from constrained counting. Instead, recent work \cite{KCMR18} gave heuristics that can sometimes find a ``good-enough'' contraction order through structure-based optimization. Finding efficient contraction orders for tensor networks remains an area of active research \cite{RTPCTSL19}.


The primary contribution of this chapter is the application of heuristic graph-decomposition techniques to find efficient contraction orders for tensor networks. Algorithms based on graph decompositions have been successful across computer science \cite{GLST17,MPPV04}, and their success in practice relies on finding good decompositions of arbitrary graphs. This, along with several recent competitions \cite{DKTW18}, has spurred the development of a variety of heuristics and tools for efficiently finding graph decompositions \cite{AMW17,HS18,Tamaki17}. While we do not establish new parameterized complexity results for model counting (as fixed-parameter algorithms for model counting are well-known for a variety of parameters \cite{FMR08,SS10}), we combine these theoretical results with high-performance tensor network libraries and with existing heuristic graph-decomposition tools to produce a competitive tool for weighted model counting.

We first discuss the \textbf{Line-Graph} method (\textbf{LG}) for finding efficient contraction orders through structure-based graph analysis. First applied to tensor networks by Markov and Shi \cite{MS08}, we contribute a new analysis that more closely matches the memory usage of existing tensor libraries. Our analysis combines two theoretical insights: (1) memory-efficient contraction orders are equivalent to low-width carving decompositions (first observed in \cite{de15}), and (2) tree decompositions can be used to find carving decompositions. \textbf{LG} has previously been implemented using exact tools for finding tree decompositions \cite{DFGHSW18}, but its implementation using heuristic tools for tree decompositions is largely unexplored.
% Moreover, our analysis implies that memory-optimal contraction orders for planar tensor networks can be found in cubic time.

Although \textbf{LG} is a general-purpose method for finding contraction orders, \textbf{LG} cannot handle high-rank tensors and so cannot solve many existing counting benchmarks. We therefore contribute a novel structure-based method for finding efficient contraction orders, tailored for constrained counting: the \textbf{Factor-Tree} method (\textbf{FT}). \textbf{FT} factors high-rank, highly-structured tensors as a preprocessing step, leveraging prior work on Hierarchical Tucker representations \cite{Grasedyck10}.

In order to compare our approaches against other model counters (weighted counters \tool{cachet} \cite{SBK05}, \tool{miniC2D} \cite{OD15}, and \tool{d4} \cite{LM17}, and unweighted counters \tool{dynQBF} \cite{CW16}, \tool{dynasp} \cite{FHMW17}, and \tool{SharpSAT} \cite{Thurley2006}), as well as other tensor-based methods, we implemented \textbf{LG} and \textbf{FT} using three state-of-the-art heuristic tree-decomposition solvers. The resulting new weighted model counter is called \tool{TensorOrder}. \textbf{LG} outperforms other model counters and tensor-based methods on a set of unweighted benchmarks, while \textbf{FT} improves the virtual best solver on 21\% of a standard set of weighted model counting benchmarks. \tool{TensorOrder} is thus useful as part of a portfolio of weighted model counters. All code, benchmarks, and detailed data of benchmark runs are available at \url{https://github.com/vardigroup/TensorOrder}.


The rest of the chapter is organized as follows: we provide graph notations and define graph decompositions in Section~\ref{sec:tensors:prelim}. We introduce tensors and tensor networks and discuss prior work on the optimization of tensor-network contraction in Section~\ref{sec:tensors:tensors}. We introduce a framework for solving the problem of weighted model counting with tensor networks in Section~\ref{sec:tensors:wmc}. We discuss the \textbf{Line-Graph} method in Section~\ref{sec:tensors:contraction-theory} and the \textbf{Factor-Tree} method in Section~\ref{sec:tensors:preprocessing}. We present an experimental evaluation of tensor-based approaches to model counting in Section~\ref{sec:tensors:experiments}. Finally, we discuss future work and conclude in Section~\ref{sec:tensors:conclusion}.