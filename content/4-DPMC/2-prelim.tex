\section{Preliminaries}
\label{sec_prelim}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{\textbf{Pseudo-Boolean Functions and Early Projection}}

A \emph{pseudo-Boolean function} over a set $X$ of variables is a function $f: 2^X \to \R$.
Operations on pseudo-Boolean functions include \emph{product} and \emph{projection}.
First, we define product.
\begin{definition}[Product]
\label{def_mult}
    Let $X$ and $Y$ be sets of Boolean variables.
    The \textdef{product} of functions $f: 2^X \to \R$ and $g: 2^Y \to \R$ is the function $f \mult g: 2^{X \cup Y} \to \R$ defined for all $\tau \in 2^{X \cup Y}$ by
    $(f \mult g)(\tau) \equiv f(\tau \cap X) \mult g(\tau \cap Y).$
\end{definition}

Next, we define (additive) projection, which marginalizes a single variable.
\begin{definition}[Projection]
\label{def_proj}
    Let $X$ be a set of Boolean variables and $x \in X$.
    The \textdef{projection} of a function $f: 2^X \to \R$ \wrt{} $x$ is the function $\proj_x f: 2^{X \setminus \set{x}} \to \R$ defined for all $\tau \in 2^{X \setminus \set{x}}$ by
    $\pars{\proj_x f}(\tau) \equiv f(\tau) + f(\tau \cup \set{x}).$
\end{definition}
Note that projection is commutative, \ie, that $\proj_x \proj_y f = \proj_y \proj_x f$ for all variables $x, y \in X$ and functions $f: 2^X \to \R$.
Given a set $X = \set{x_1, x_2, \ldots, x_n}$, define
$\proj_X f \equiv \proj_{x_1} \proj_{x_2} \ldots \proj_{x_n} f$.
Our convention is that $\proj_\emptyset f \equiv f$.

When performing a product followed by a projection, it is sometimes possible to perform the projection first.
This is known as \emph{early projection} \cite{mcmahan2004projection}.
\begin{theorem}[Early Projection]
\label{thm_early_proj}
    Let $X$ and $Y$ be sets of variables.
    For all functions $f: 2^X \to \R$ and $g: 2^Y \to \R$, if $x \in X \setminus Y$, then $\proj_x (f \mult g) = \pars{\proj_x f} \mult g.$
    % As a corollary, for all $X' \subseteq X \setminus Y$,
    % $$\exists_{X'} (A \mult B) = \pars{\exists_{X'} A} \mult B.$$
\end{theorem}
Early projection is a key technique in symbolic computation in a variety of settings, including database-query optimization \cite{kolaitis2000conjunctive}, symbolic model checking \cite{burch1991symbolic}, satisfiability solving \cite{pan2005symbolic}, and model counting \cite{dudek2020addmc}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{\textbf{Weighted Model Counting}}

We compute the total weight, subject to a given weight function, of all models of an input propositional formula.
Formally:
\begin{definition}[Weighted Model Count]
    Let $X$ be a set of Boolean variables, $\phi: 2^X \to \{0,1\}$ be a Boolean function, and $W: 2^X \to \R$ be a pseudo-Boolean function.
    The \textdef{weighted model count} of $\phi$ \wrt{} $W$ is
    $W(\phi) \equiv \sum_{\tau \in 2^X} \phi(\tau) \mult W(\tau)$.
\end{definition}

The weighted model count of $\phi$ \wrt{} $W$ can be naturally expressed in terms of pseudo-Boolean functions: $W(\phi) = \left(\sum_X (\phi \mult W) \right)(\emptyset)$.
The function $W: 2^X \to \R$ is called a \textdef{weight function}.
In this work, we focus on {literal-weight functions}, which can be expressed as products of weights associated with each variable.
Formally, a \textdef{literal-weight function} $W$ can be factored as $W = \prod_{x \in X} W_x$ for pseudo-Boolean functions $W_x: 2^{\{x\}} \to \R$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{\textbf{Graphs}}

A \emph{graph} $G$ has a
% nonempty % Vu needs an initially empty tree
set $\V{G}$ of vertices, a set $\E{G}$ of (undirected) edges, a function $\delta_G: \V{G} \to 2^{\E{G}}$ that gives the set of edges incident to each vertex, and a function $\epsilon_G: \E{G} \to 2^{\V{G}}$ that gives the set of vertices incident to each edge.
Each edge must be incident to exactly two vertices, but multiple edges can exist between two vertices.
A \emph{tree} is a simple, connected, and acyclic graph.
A \emph{leaf} of a tree $T$ is a vertex of degree one, and we use $\Lv{T}$ to denote the set of leaves of $T$.
We often refer to a vertex of a tree as a \emph{node} and an edge as an \emph{arc} to avoid confusion.
A \emph{rooted tree} is a tree $T$ together with a distinguished node $r \in \V{T}$ called the \emph{root}.
In a rooted tree $(T, r)$, each node $n \in \V{T}$ has a (possibly empty) set of \emph{children}, denoted $\C{T}{r}{n}$, which contains all nodes $n'$ adjacent to $n$ \st{} all paths from $n'$ to $r$ contain $n$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \paragraph\textbf{{Algebraic Decision Diagrams}}

% A \emph{Binary Decision Diagram}, or BDD, is a data structure that represents a Boolean function $f : \{0, 1\}^n \to \{0, 1\}$ as a directed acyclic graph \cite{bryant1986graph}.
% BDDs have previously been applied to satisfiability checking \cite{bryant1992symbolic,pan2004search} and constrained counting \cite{lobbing1996number,minato1993zero}, but these approaches fail to generalize to large problem instances.
% In particular, these approaches for constrained counting require constructing a monolithic (possibly exponentially-sized) BDD as an intermediate step and so quickly become infeasible in practice.
% An \emph{Algebraic Decision Diagram}, or ADD, is a generalization of a BDD that allows for non-Boolean values at leaf nodes \cite{bahar1997algebric}.
% An ADD represents a function $f : \{0, 1\}^n \to S$ for an arbitrary domain $S$.
% ADDs have been successfully applied to solve propositionally factored Markov Decision Processes \cite{hoey1999spudd} and in model checking \cite{clarke2018model,hansen2002symbolic}.
% There are libraries (\eg, \cite{somenzi2015cudd}) that efficiently implement a variety of operations on ADDs, including product and projection.

%% Vu: the following paragraph reveals our identities and is unsuitable for blind review
% Although Binary Decision Diagrams (BDDs) have been previously used for constrained counting \cite{lobbing1996number,minato1993zero}, these approaches require constructing a monolithic (possibly exponentially-sized) BDD representing the input formula $F$ as an intermediate step and so quickly become infeasible in practice.
% In recent work \cite{dudek2020addmc} We proposed to modify these approaches to construct an ADD representing the formula $F$, with the weight function giving the value of each leaf node of the ADD.
% To make this approach successful, we developed algebraic techniques of ``early quantification'', for ADDs, extending those developed for BDDs \cite{burch1991symbolic,tabajara2017factored}.
% The goal is to iteratively reduce the size of the ADD during the construction and therefore avoid constructing the entire ADD at once.
% We showed that this approach provides significant speedups over prior techniques that use BDDs for constrained counting.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \paragraph\textbf{{Tensors}}

% A tensor is a multi-dimensional generalization of a matrix.
% Tensor are widely used in data analysis \cite{cichocki2014era}, signal and image processing \cite{cichocki2015tensor}, quantum physics \cite{arad2010quantum}, quantum chemistry \cite{smilde2005multi}, and many other areas of science.
% Given the diverse applications of tensors and tensor networks, a variety of tools \cite{baumgartner2005synthesis,kjolstad2017tensor} exist to manipulate them efficiently.

% Tensors can be used to represent pseudo-Boolean functions in a dense way.
% Tensors are particularly efficient at computing the \emph{contraction} of two pseudo-Boolean functions: given two functions $f: 2^X \to \R$ and $g: 2^Y \to \R$, their contraction is the pseudo-Boolean function $\proj_{X \cap Y} f \cdot g$.
% The contraction of two tensors can be implemented as matrix multiplication and so leverage significant work in high-performance computing on matrix multiplication on CPUs \cite{lawson1979basic} and GPUs \cite{fatahalian2004understanding}.

% A tensor network is a representation of a complex tensor by interconnected, simpler tensors.
% The problem of \emph{tensor-network contraction}, given a tensor network, is to compute explicitly the tensor represented by the network.
% For a more formal definition of tensors, tensor networks, and tensor-network contraction, see \cite{arad2010quantum}.
% Given the diverse applications of tensors and tensor networks, a variety of tools \cite{baumgartner2005synthesis,kjolstad2017tensor} exist to manipulate them efficiently.

% An algorithm to reduce constrained counting (with a constant weight function) to tensor-network contraction was proposed recently \cite{biamonte2015tensor}.
% The analysis of this algorithm has so far been purely theoretical, focusing, particularly, on identifying classes of formulas $F$ for which the corresponding tensor-network contraction can be performed in polynomial time \cite{biamonte2015tensor}.
% We have recently explored implementing this algorithm on top of state-of-the-art tools for tensor-network contraction \cite{baumgartner2005synthesis,kjolstad2017tensor} to obtain a tool for constrained counting \cite{dudek2019efficient}.

% Algorithms for tensor network contraction rely on finding a good order to contract the tensors of the tensor network.
% This problem is closely connected to join-query optimization in database theory \cite{wong1976decomposition,mcmahan2004projection}, but this connection was largely unexplored by prior work.
% The connection between Boolean satisfiability (and hence constrained counting) and join-query optimization has been explored previously \cite{dalmau2002constraint}.
% By improving the connection between tensor-network contraction and constrained counting, we aimed to use results from join-query optimization (in particular, on using graph decompositions \cite{greco2018structural,mcmahan2004projection}) to improve tensor-network contraction both for constrained counting and for more general problems.
% We showed tensor networks are a significant novel addition to the portfolio of constrained-counting algorithms.
