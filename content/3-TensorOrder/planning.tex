\section{Planning Phase: Finding a Contraction Tree}
The problem of \emph{tensor-network-contraction optimization} is given a tensor network $N$ to find a contraction tree that minimizes the computational cost of Algorithm \ref{alg:network-contraction}. % Prior work can be categorized into two types: \emph{exact} algorithms that focus on finding the best possible contraction tree, and \emph{heuristic} algorithms that aim to find a ``good-enough'' contraction tree. Since the number of possible contraction trees is exponential in the number of tensors, exact algorithms are generally best for tensor networks with few tensors while heuristic algorithms scale to larger tensor networks but offer worse contraction trees.
Several \emph{cost-based} approaches aim for minimizing the total number of floating point operations to perform Algorithm \ref{alg:network-contraction} in step 8, e.g. \cite{PHV14} and the \pkg{einsum} package in \pkg{numpy}.  % A more complex cost-based approach is the Tensor Contraction Engine \cite{Hirata03}, which performs additional optimizations of Algorithm \ref{alg:network-contraction} at the compiler level. 
In this section, we instead focus on \emph{structure-based} approaches to tensor-network-contraction optimization, which analyze the rank of intermediate tensors that appear during Algorithm \ref{alg:network-contraction}. 
These ranks indicate the amount of memory and computation required at each recursive stage. Moreover, these ranks are more amenable to analysis.

One line of work \cite{MS08,DFGHSW18} uses graph decompositions to analyze the \emph{contraction complexity} of a contraction tree: the maximum over all recursive calls of the sum (minus 1) of the rank of the two tensors contracted in step 8 of Algorithm \ref{alg:network-contraction}. In our factor graph analogy, contraction complexity of a contraction tree corresponds to the \emph{width} of a dtree (i.e., the size of the largest cluster) \cite{darwiche01b}.
Contraction complexity measures the memory required when step 8 is computed by summing over the shared indices separately, i.e., first contracting $A_1$ and $A_2$ as if they have no common indices (producing and storing a rank $|\tdim{A_1}|+|\tdim{A_2}|$ tensor) and then sequentially contracting each pair of shared indices. In this approach, the memory required is exponential in the contraction complexity. Modern tensor packages (e.g. \pkg{numpy}), however, instead sum over all shared indices simultaneously without storing an intermediate result. This alternative approach requires the same number of floating-point operations, but often requires significantly less intermediate memory. Thus contraction complexity overestimates the memory requirements of many contraction trees. 

Instead, another line of structure-based optimization analyzes the maximum rank over all recursive calls of the result of step 8 (and step 3). We call this the \emph{max rank} of a contraction tree. In our factor graph analogy, max rank of a contraction tree corresponds to the size of the largest separator of a dtree \cite{darwiche01b} and is closely connected to the performance of the Shenoy-Shafer algorithm \cite{shenoy97,SS08}. Max rank measures the memory required when step 8 is computed by summing over all shared indices simultaneously without storing an intermediate result: the memory required is exponential in the max rank. Thus, max rank estimates the memory usage of modern tensor packages. While the max rank is no more than linearly smaller than the contraction complexity, both appear in the exponent of the required memory. Therefore even small differences between max rank and contraction complexity result in significantly different memory usage. Recent work \cite{KCMR18} introduced three methods for heuristically minimizing the max rank: a greedy approach (called \textbf{greedy}), an approach using graph partitioning (called \textbf{metis}), and an approach using community-structure detection (called \textbf{GN}). Note that \textbf{metis} is analogous to earlier work on using graph partitioning to construct dtrees with small separators \cite{darwiche01b}.

\input{content/3-TensorOrder/theory_line_graph}
\input{content/3-TensorOrder/theory_factor_tree}

