\chapter{Introduction}
\label{ch:intro}
Discrete integration is a fundamental problem in artificial intelligence, with applications in probabilistic reasoning, planning, inexact computing, engineering reliability, and statistical physics \cite{Bacchus2003,DH07,GSS08,naveh2007constraint}. 
In discrete integration (also called weighted model counting) the task is to count the total weight, subject to a given weight function, of the set of solutions of input constraints.
Even when the weight function is a constant function, constrained counting is \#P-Complete \cite{Valiant79}. 
Nevertheless, a variety of tools exist that can handle industrial sets of constraints, \eg, \cite{sang2004combining,OD15,darwiche2004new,LM17}. 


Dynamic programming is a powerful technique that has been applied across computer science \cite{bellman1966dynamic}, including to model counting \cite{BDP09,SS10,jegou2016improving}.
The key idea is to solve a large problem by solving a sequence of smaller subproblems and then incrementally combining these solutions into the final result.
Dynamic programming provides a natural framework to solve a variety of problems defined on sets of constraints: subproblems can be formed by partitioning the constraints.
This framework has been instantiated into algorithms for database-query optimization \cite{MPPV04}, satisfiability solving \cite{uribe1994ordered,aguirre2001random,pan2005symbolic}, and QBF evaluation \cite{charwat2016bdd}. 
Dynamic programming has also been the basis of a variety of algorithms \cite{FMR08,SS10} and tools \cite{CW16,FHMW17,FHWZ18,FHZ19,DPV20,fichte2020exploiting} for model counting.

The key idea of this thesis is that dynamic-programming algorithms for model counting can be cleanly separated into two phases: a \emph{planning phase} of high-level reasoning, followed by an \emph{execution phase} of low-level computation. 
The requirements, challenges, and opportunities of each phase are often dramatically different, both at an algorithmic and an implementation level.  
This separation enables us to separately reason about, implement, and optimize both aspects of the algorithm independently.
This thesis argues that this separation enables the resulting model counters to scale to large problem instances and run flexibly in a variety of hardware environments.

In the planning phase, we aim to leverage graph decompositions as a tool for high-level reasoning.
Algorithms based on graph decompositions have been successful across computer science \cite{GLST17,MPPV04}, and their success in practice relies on finding good decompositions of arbitrary graphs. 
This, along with several recent competitions \cite{DKTW18}, has spurred the development of a variety of heuristics and tools for efficiently finding graph decompositions \cite{AMW17,HS18,Tamaki17,hicks02}. 
A variety of algorithms \cite{FMR08,SS10} and tools \cite{CW16,FHMW17,FHWZ18,FHZ19} exist for performing model counting using graph decompositions.

In the execution phase, we aim to leverage existing low-level computational libraries. 
We primarily focus on two directions: tensors and decision diagrams. 
\emph{Tensors} are a tool used across quantum physics and computer science for describing and reasoning about quantum systems, big-data processing, and more \cite{BB17,Cichocki14,Orus19}. 
There is massive practical work across machine learning and high-performance computing on tensor contraction \cite{BK07,Hirata03,KKCLA17,VZTGDMVAC18} (which also includes GPU support \cite{KSTKPPRS19,NRBHHJN15}).

% While we do not establish new parameterized complexity results for model counting (as fixed-parameter algorithms for model counting are well-known for a variety of parameters \cite{FMR08,SS10}), we combine these theoretical results with high-performance tensor network libraries and with existing heuristic graph-decomposition tools to produce a competitive tool for weighted model counting.

% The parallelization of neural network training and inference has seen massive research across the machine learning and high-performance computing communities \cite{ABCCDDDGII16,JYPPABBBBB17,PGMLJGKLGA19}. Consequently, GPUs give orders of magnitude of speedup over a single core for neural-network algorithms \cite{KSTKPPRS19,NRBHHJN15}. In this work, we aim to directly leverage advances in multi-core and GPU performance designed for neural-network algorithms in the service of weighted model counting.


% Dynamic programming has also been the basis of several tools for model counting \cite{DPV20,DDV19,dudek2020parallel,fichte2020exploiting}.
% Although each tool uses a different data structure--algebraic decision diagrams (ADDs) \cite{DPV20}, tensors \cite{DDV19,dudek2020parallel}, or database tables \cite{fichte2020exploiting}--the overall algorithms have similar structure.
% % Also, decomposition techniques have seen many applications in model counting, cf.~\cite{jegou2016improving} and the reference therein. %JD: Moved the citation to paragraph 2 instead
% The goal of this work is to unify these approaches into a single conceptual framework: \emph{project-join trees}.
% Project-join trees are not an entirely new idea.
% Similar concepts have been used in constraint programming (as join trees \cite{dechter1989tree}), probabilistic inference (as cluster trees \cite{SAS94}), and database-query optimization (as join-expression trees \cite{MPPV04}).
% Our original contributions include the unification of these concepts into project-join trees and the application of this unifying framework to model counting.

\section{Contributions}
The main contribution of this thesis is a clean separation of dynamic-programming algorithms for model counting into
Planning (high-level reasoning) and Execution (low-level computation) phases.

We first introduce this separation in a new model counter, \tool{TensorOrder}. This includes a new reduction from weighted model counting to tensor network contraction. We also introduce a novel planning algorithm to factor tensor networks based on tree decompositions.

Next, we show that introducing this separation improves an existing model counter (\tool{ADDMC} \cite{DPV20}). We unify a variety of approaches into a single conceptual framework using project-join trees. We show that replacing the existing constraint-satisfaction planner with a planner based on graph decompositions leads to a faster model counter.

We next use this approach to develop a parallel model counter by separately parallelizing the planning and execution phases. To handle limited-memory environments (e.g., on a GPU), we introduce a novel technique for parallel model counting by using variable marginalization.

Finally, we extend our approach to projected model counting. We present a novel algorithm for performing projected model counting through a planning and execution phase. As part of this, we introduce a novel planning algorithm that builds graded project-join trees from tree decompositions.

\section{Organization}
The remainder of this thesis is organized as follows:

Chapter \ref{ch:background} presents notation and background on model counting, existing high-level planning tools (graph decompositions), and existing low-level computational tools (tensors and decision diagrams). 

Chapter \ref{ch:tensors} presents a model counter \tool{TensorOrder} that uses graph decompositions for planning and tensors for execution. Most results in this chapter appear in \cite{DDV19}.

Chapter \ref{ch:dpmc} presents a model counter \tool{DPMC} that uses graph decompositions for planning and decision diagrams for execution. Most results in this chapter appear in \cite{dudek2020dpmc}.

Chapter \ref{ch:parallel} parallelizes the techniques from Chapter \ref{ch:tensors} to run on multiple CPUs, a GPU, and on a TPU. Most results in this chapter appear in \cite{dudek2020parallel}.

Chapter \ref{ch:procount} generalizes the techniques from Chapter \ref{ch:dpmc} to projected model counting.

Finally, Chapter \ref{ch:conclusion} summarizes the main contributions of this thesis and describes several possible future directions.