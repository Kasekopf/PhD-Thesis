\section{An Introduction to Tensors and Tensor Networks}
\label{sec:tensors:tensors}
In this section, we introduce tensors, tensor networks, and the problem of tensor-network contraction. To aid in exposition, along the way we build an analogy between the language of databases \cite{SG88}, the language of factor graphs \cite{KFL01,dechter99,darwiche01b}, and the language of tensors: see Table \ref{table:db-tensor-analogy}.

\begin{table}[t]
\centering
\begin{tabular}{c|c|c}
\hline
\textbf{Database Concept} & \textbf{Factor Graph Concept} & \textbf{Tensor Concept}\\ \hline
Attribute & Variable & Index\\
Table & Pseudo-Boolean Function & Tensor\\
Project-Join Query & Factor Graph & Tensor Network\\
Join Tree & Dtree & Contraction Tree\\ 
Width & Largest Cluster Size & Contraction Complexity \\
- & Largest Separator Size & Max Rank \\ \hline
\end{tabular}
\caption{\label{table:db-tensor-analogy} An analogy between the language of databases, the language of factor graphs, and the language of tensors.}
\end{table}

\subsection{Tensors}
\emph{Tensors} are a generalization of vectors and matrices to higher dimensions-- a tensor with $r$ dimensions is a table of values each labeled by $r$ indices. An index is analogous to a variable in constraint satisfaction or an attribute in database theory. 

Fix a set $\Ind$ and define an \emph{index} to be an element of $\Ind$. For each index $i$ fix a finite set $\domain{i}$ called the \emph{domain} of $i$. An index index is analogous to a variable in constraint satisfaction. %For the algorithms in this work it is sufficient for every index to have domain $\{0, 1\}$.

An \emph{assignment} to a set of indices $I \subseteq \Ind$ is a function $\tau$ that maps each index $i \in I$ to an element of $\domain{i}$. Let $\domain{I}$ denote the set of assignments to $I$, i.e., $$\domain{I} = \{\tau: I \rightarrow \bigcup_{i \in I} \domain{i}~\text{s.t.}~\tau(i) \in \domain{i}~\text{for all}~i \in I\}.$$

% Formally, fix a finite set $\Ind$ and finite sets $\domain{i}$ for each $i \in \Ind$. An \emph{index} is an element $i$ of $\Ind$, and the corresponding set $\domain{i}$ is called the \emph{domain} of $i$. 
%Fix a finite set $\Ind$, whose elements we call \emph{indices}, and for each index fix a finite set $\domain{i}$ called the \emph{domain} of $i$. 

We now define tensors as multidimensional arrays of values, indexed by assignments to a set of indices:\footnote{In some works, a tensor is defined as a multilinear map and Definition \ref{def:tensor} would be its representation in a fixed basis.}
\begin{definition}[Tensor] \label{def:tensor}
	A \emph{tensor} $A$ over a finite set of indices (denoted $\tdim{A}$) is a function $A: \domain{\tdim{A}} \rightarrow \mathbb{C}$ (where $\mathbb{C}$ is the set of complex numbers).
\end{definition}

The \emph{rank} of a tensor $A$ is the cardinality of $\tdim{A}$. The memory to store a tensor (in a dense way) is exponential in the rank. For example, a scalar is a rank 0 tensor, a vector is a rank 1 tensor, and a matrix is a rank 2 tensor. An example of a higher-rank tensor is the \emph{copy tensor} on a set of indices $I$, which is the tensor $\copyt_I: \domain{I} \rightarrow \mathbb{C}$ such that, for all $\tau \in \domain{I}$, $\copyt_I(\tau) \equiv 1$ if $\tau$ is a constant function on $I$ and $\copyt_I(\tau) \equiv 0$ otherwise \cite{BCJ11}.
Note that a pseudo-Boolean function can be seen as a special case of a tensor, where each index has a domain of size 2 and every tensor entry lies in $\mathbb{R}$.

It is common to consider sets of tensors closed under contraction (see Section \ref{sec:tensors:tensors:tensor-networks}), e.g. tensors with entries in $\mathbb{R}$ as in Section \ref{sec:tensors:wmc}. Database tables under bag-semantics \cite{CV93}, i.e., multirelations, are tensors with entries in $\mathbb{N}$. Probabilistic database tables \cite{CP87} are tensors with entries in $[0, 1]$ that sum to 1.

Many tools exist (e.g. \pkg{numpy} \cite{numpy}) to efficiently manipulate tensors. In Section \ref{sec:tensors:experiments}, we use these tools to implement tensor-network contraction, defined next.

% We abuse notation and allow for two tensors to be treated as distinct even if they are equal as functions.

%Given two tensors with possibly overlapping sets of indices, a natural operation is the contraction of the two tensors.
%\begin{definition}[Tensor Contraction]
%	Let $A$ and $B$ be tensors. The \emph{contraction} of $A$ and $B$, denoted $A \cdot B$, is the tensor $A \cdot B: \domain{\tdim{A} \oplus \tdim{B}} \rightarrow \mathbb{C}$ defined by
%	$$\tau \mapsto \sum_{\rho \in \domain{\tdim{A} \cap \tdim{B}}} A(\rho \cup \tau\restrict{\tdim{A}}) \cdot B(\rho \cup \tau\restrict{\tdim{B}}).$$
%\end{definition}
% For example, given two matrices represented as rank 2 tensors with a single index in common, the contraction of the two tensors is equivalent to matrix multiplication. The contraction of two copy tensors $\copyt_I$ and $\copyt_J$ (with $I \cap J \neq \emptyset$) is the copy tensor $\copyt_{I \oplus J}$.

\subsection{Tensor Networks}
\label{sec:tensors:tensors:tensor-networks}
A \emph{tensor network} defines a complex tensor by combining a set of simpler tensors in a principled way. This is analogous to how a database query defines a resulting table in terms of a computation across many tables.

\begin{definition}[Tensor Network]
	\label{def:tensor-contraction-network}
	A \emph{tensor network} $N$ is a nonempty set of tensors across which no index appears more than twice.
\end{definition}

\emph{Free indices} of $N$ are indices that appear once, while \emph{bond indices} of $N$ are indices that appear twice. We denote the set of free indices of $N$ by $\tnfree{N}$ and the set of bond indices of $N$ by $\tnbound{N}$. The \emph{bond dimension} of $N$ is the maximum size of $\domain{i}$ for all bond indices $i$ of $N$.

The problem of \emph{tensor-network contraction}, given an input tensor network $N$, is to compute the \emph{contraction} of $N$ by marginalizing all bond indices:
\begin{definition}[Tensor Network Contraction]\label{def:contraction}
The \emph{contraction} of a tensor network $N$ is a tensor $\tntensor{N}$ with indices $\tnfree{N}$ (the set of free indices of $N$), i.e. a function $\tntensor{N} : \domain{\tnfree{N}} \rightarrow \mathbb{C}$, that is defined for all $\tau \in \domain{\tnfree{N}}$ by
		\begin{equation}
        \tntensor{N}(\tau) \equiv \sum_{\rho \in \domain{\tnbound{N}}} \prod_{A \in N} A((\rho \cup \tau)\restrict{\tdim{A}}).
        \end{equation}
\end{definition}

For example, the contraction of the tensor network $\{\copyt_I, \copyt_J\}$ with $I \cap J \neq \emptyset$ is the tensor $\copyt_{I \oplus J}$ (where $I \oplus J$ is the symmetric difference of $I$ and $J$). Notice that if a tensor network has no free indices then its contraction is a rank 0 tensor. 

A tensor network $N'$ is a \emph{partial contraction} of a tensor network $N$ if there is a surjective function $f: N \rightarrow N'$ s.t. for every $A \in N'$ we have $\tntensor{f^{-1}(A)} = A$; that is, if every tensor in $N'$ is the contraction of some tensors of $N$. If $N'$ is a partial contraction of $N$, then $\tntensor{N'} = \tntensor{N}$.

Let $A$ and $B$ be tensors. Their \emph{contraction} $A \cdot B$ is the contraction of the tensor network $\{A, B\}$. If $\tdim{A} = \tdim{B}$, their \emph{sum} $A+B$ is the tensor with indices $\tdim{A}$ whose entries are given by the sum of the corresponding entries in $A$ and $B$.

Following our analogy, given a tensor network containing database tables (under bag-semantics) as tensors, its contraction is the join of those tables followed by the projection of all shared attributes. Thus a tensor network is analogous to a project-join query. A tensor network can also be seen as a variant of a factor graph \cite{KFL01} with the additional practical restriction that no variable appears more than twice. The contraction of a tensor network corresponds to the marginalization of a factor graph \cite{RS17}, which is a a special case of the sum-of-products problem \cite{BDP09,dechter99} and the FAQ problem \cite{KNR16}. The restriction on variable appearance is heavily exploited in tools for tensor-network contraction and in this work, since it allows tensor contraction to be implemented as matrix multiplication and so leverage significant work in high-performance computing on matrix multiplication on CPUs \cite{LHKK77} and GPUs \cite{FSH04}.

% TODO mention this
% We focus in this work on tensor networks with relatively few (or no) free indices and hundreds or thousands of bond indices. Such tensor networks are obtained in a variety of applications \cite{Cichocki14,DLVR18}, including the reduction from model counting to tensor network contraction \cite{BMT15}.