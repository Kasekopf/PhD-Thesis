\section{Execution Phase: Parallelizing with Index Slicing}
\label{sec:parallel:execution}
As discussed in Chapter \ref{ch:background}, each contraction in Algorithm \ref{alg:network-contraction} can be implemented as a matrix multiplication.
This was done in Chapter \ref{ch:tensors} using \tool{numpy} \cite{numpy}, and it is straightforward to adjust the implementation to leverage multiple cores with \tool{numpy} or leverage a GPU with \tool{TensorFlow} \cite{ABCCDDDGII16}. 
In Section \ref{sec:parallel:execution:gpu}, we discuss challenges in dealing with GPU on-board memory and our solution of index slicing. In Section \ref{sec:parallel:execution:tpu}, we then discuss how index slicing allows us to efficiently run the execution phase on a TPU using \tool{jax} \cite{jax2018github}.

\subsection{Contraction on a GPU}
\label{sec:parallel:execution:gpu}
The primary challenge that emerges when using a GPU is dealing with the small onboard memory. For example, the \tool{NVIDIA Tesla-V100} (which we use in Section \ref{sec:experiments}) has just 16GB of onboard memory. This limits the size of tensors that can be easily manipulated. One solution is to perform a single contraction of two tensors cross multiple GPU kernel calls \cite{RRBSKH08}. This is similar to the technique implemented in \tool{gpusat2} \cite{FHZ19}. This solution, however, require parts of the input tensors to be copied into and out of GPU memory multiple times for each contraction, which incurs significant slowdown.

Instead, we use \emph{index slicing} \cite{CZHNS18,GK20,VBNHRBM19} of a tensor network $N$. This technique is analogous to \emph{conditioning} on Bayesian networks \cite{darwiche01,dechter99,pearl86,SAS94}. The idea is to represent $\tntensor{N}$ as the sum of contractions of smaller tensor networks. Each smaller network contains tensor slices:

%The main it is to contract a tensor network $N$ by choosing a subset of bond indices $I \subseteq \tnbound{N}$ and representing $\tntensor{N}$ into the sum of $|\domain{I}|$ contractions of smaller tensor networks. % This corresponds to moving the marginalization of the indices in $I$ to the outside of Equation \ref{eqn:contraction}.
%These smaller tensor networks contain tensor slices:
\begin{definition}
Let $A$ be a tensor, $I \subseteq \textbf{Ind}$, and $\eta \in \domain{I}$. Then the \emph{$\eta$-slice} of $A$ is the tensor $A[\eta]$ with indices $\tdim{A} \setminus I$ defined for all $\tau \in \domain{\tdim{A} \setminus I}$ by $A[\eta](\tau) \equiv A((\tau \cup \eta)\restrict{\tdim{A}}).$
\end{definition}

These are called slices because every value in $A$ appears in exactly one tensor in $\{ A[\eta] : \eta \in \domain{I} \}$. We now define and prove the correctness of index slicing: 
\begin{theorem}
Let $N$ be a tensor network and let $I \subseteq \tnbound{N}$. For each $\eta \in \domain{I}$, let $N[\eta] = \{ A[\eta] : A \in N\}$. Then $\tntensor{N} = \sum_{\eta \in \domain{I}} \tntensor{N[\eta]}.$
\end{theorem}
\begin{proof}
Move the summation over $\eta \in \domain{I}$ to the outside of Equation \ref{eqn:contraction}, then apply the definition of tensor slices and recombine terms.
\end{proof}

\begin{algorithm*}[t]
    \caption{Sliced contraction of a tensor network}
    \label{alg:tn-sliced}
    \DontPrintSemicolon
    \KwIn{$N$: a tensor network}
    \KwIn{$T$: a contraction tree for $N$}
    \KwIn{$m$: a memory bound}
    \KwOut{$\tntensor{N}$, the contraction of $N$, performed using at most $m$ memory.}
    \Function{\upshape $\func{ExecuteSliced}(N, T, m)$}{
        $I \gets \emptyset$\;
        \While{$\func{MemCost}(N, T, I) > m$}{
            $I \gets I \cup \{\func{ChooseSliceIndex}(N, T, I)\}$\;
        }
        \Return{$\sum_{\eta \in [I]} \func{Execute}(N[\eta], T[\eta])$} \label{line:tn-sliced:execute}
    }
\end{algorithm*}

By choosing $I$ carefully, computing each $\tntensor{N[\eta]}$ uses less intermediate memory (compared to computing $\tntensor{N}$) while using the same contraction tree. In exchange, the number of floating point operations to compute all $\tntensor{N[\eta]}$ terms increases. 

Choosing $I$ is itself a difficult problem. Our goal is to choose the smallest $I$ so that contracting each network slice $N[\eta]$ can be done in onboard memory. We first consider adapting Bayesian network conditioning heuristics to the context of tensor networks. Two popular conditioning heuristics are (1) \emph{cutset conditioning} \cite{pearl86}, which chooses $I$ so that each network slice is a tree, and (2) \emph{recursive conditioning} \cite{darwiche01}, which chooses $I$ so that each network slice has a disconnected structure graph\footnote{The full recursive conditioning procedure then recurses on each connected component. While recursive conditioning is an any-space algorithm, the partial caching required for this is difficult to implement on a GPU.}. Both of these heuristics result in a choice of $I$ far larger than our goal requires. 
% Cutset conditioning chooses $I$ much larger than needed for our purpose, while recursive conditioning scales in memory using an explicit 


Instead, in this work as a first step we use a heuristic from \cite{CZHNS18,GK20}: choose $I$ incrementally, greedily minimizing the memory cost of contracting $N[\eta]$ until the memory usage fits in onboard memory. Unlike cutset and recursive conditioning, the resulting networks $N[\eta]$ typically still have highly connected structure graphs. One direction for future work is to compare other heuristics for choosing $I$ (e.g., see the discussion in Section 10 of \cite{dechter99}).

This gives us Algorithm \ref{alg:tn-sliced}, which performs the execution phase with limited memory at the cost of additional time. $T[\eta]$ is the contraction tree obtained by computing the $\eta$-slice of every tensor in $T$. $\func{MemCost}(N, T, I)$ computes the memory for one $\func{Execute}(N[\eta], T[\eta])$ call. $\func{ChooseSliceIndex}(N,T,I)$ chooses the next slice index greedily to minimize memory cost.

\subsection{Contraction on a TPU}
\label{sec:parallel:execution:tpu}
A TPU (tensor processing unit) \cite{JYPPABBBBB17} is a specialized computing hardware designed for neural network training and inference.
In particular, a TPU consists of multiple cores that each contain application-specific integrated circuits (ASICs) for matrix multiplication.
Several tensor libraries (e.g. \tool{TensorFlow} \cite{ABCCDDDGII16} and \tool{jax} \cite{jax2018github}) support tensor contraction on a TPU.
Thus tensor networks provide a natural framework to leverage TPUs for model counting.

The primary challenge that emerges is that computing on a TPU requires an additional compilation step.
So far, we have focused on using tensor libraries (\tool{numpy} \cite{numpy} and \tool{TensorFlow} \cite{ABCCDDDGII16}) in \emph{eager execution} mode.
In eager execution mode, a tensor library performs each tensor operation directly on a CPU or GPU using precompiled and preoptimized code.
Thus no extra compilation is needed at runtime.
Unfortunately, no current tensor libraries can leverage a TPU while in eager execution mode.

Instead, tensor libraries like \tool{TensorFlow} \cite{ABCCDDDGII16} and \tool{jax} \cite{jax2018github} support using a TPU only in \emph{graph execution} mode. 
In graph execution mode, a sequence of tensor operations on unspecified input is translated by the tensor library into an XLA graph program \cite{XLA}, which is a representation of the computation as a directed acyclic graph. 
This XLA graph program is then compiled and optimized (e.g. by modifying tensor layouts, or by fusing operations) for various hardware targets, including CPUs, GPUs, or TPUs. 

In general, the compilation step required by graph execution may take orders of magnitude longer than a single eager execution of the same computation, but a single execution of the compiled program is faster than a single eager execution. 
This trade-off is beneficial in the context of neural network training and inference, where the same computation (say, training a neural network once on a single batch of examples) is repeated many times with different inputs.
The challenge, then, is to identify repeated, independent computations in the contraction of a tensor network that may be profitably compiled.

Our key observation is that Algorithm \ref{alg:tn-sliced} computes the contraction of a tensor network through a sequence of similar, independent computations: the contraction of the sliced networks $N[\eta]$. In particular, each call to $\func{Execute}$ on Line \ref{line:tn-sliced:execute} of Algorithm \ref{alg:tn-sliced} has the same computational structure when following Algorithm \ref{alg:network-contraction}. This occurs because, for each pair $\eta, \eta' \in [I]$, there is a natural correspondence between the tensors of $N[\eta]$ and $N[\eta']$, where two tensors $A[\eta] \in N[\eta]$ and $A[\eta'] \in N[\eta']$ correspond if they are each slices of the same tensor $A \in N$. Corresponding tensors $A[\eta]$ and $A[\eta']$ have the same set of indices and differ only in their specific entries.
It follows that the computations of $\func{Execute}(N[\eta], T[\eta])$ and $\func{Execute}(N[\eta'], T[\eta'])$ proceed in lockstep in Algorithm \ref{alg:network-contraction}, differing only in the entries of the relevant tensors.

This allows us to run Line \ref{line:tn-sliced:execute} of Algorithm \ref{alg:tn-sliced} in graph execution mode on a TPU.
For a given tensor network $N$, contraction tree $T$ for $N$, and $I \subseteq \tnbound{N}$, consider the function $f_{N,T,I}$ that takes as input $\eta \in [I]$ and outputs $\func{Execute}(N[\eta], T[\eta])$.
Our approach is to represent $f_{N,T,I}$ as an XLA graph program.
We compile $f_{N,T,I}$ once, targeting a TPU, and then run the compiled program on each $\eta \in [I]$.
Note that since a TPU contains multiple cores we run the compiled program multiple times in parallel, once for each core.
We evaluate the performance of this approach in Section \ref{sec:parallel:exp:tpu}.

