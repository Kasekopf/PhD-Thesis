Most tools for discrete integration focus on single-core performance. There have been only a few parallel model counters, notably the multi-core (unweighted) model counter \tool{countAntom} \cite{BSB15} and the GPU-based weighted model counter \tool{gpuSAT2} \cite{FHWZ18,FHZ19}.
% TODO: say parallelizing model counting is hard

% In \emph{weighted model counting}, the task is to count the total weight, subject to a given weight function, of the set of solutions of input constraints. This fundamental task has applications in probabilistic reasoning, planning, inexact computing, engineering reliability, and statistical physics \cite{Bacchus2003,DH07,GSS08}. The development of model counters that can successfully compute the total weight on large industrial formulas is an area of active research \cite{OD15,Thurley2006}. Although most model counters focus on single-core performance, there have been several parallel model counters, notably the multi-core (unweighted) model counter \tool{countAntom} \cite{BSB15} and the GPU-based weighted model counter \tool{gpuSAT2} \cite{FHWZ18,FHZ19}. 

The parallelization of neural network training and inference has seen massive research across the machine learning and high-performance computing communities \cite{ABCCDDDGII16,JYPPABBBBB17,PGMLJGKLGA19}. Consequently, GPUs give orders of magnitude of speedup over a single core for neural-network algorithms \cite{KSTKPPRS19,NRBHHJN15}. In this work, we aim to directly leverage advances in multi-core and GPU performance designed for neural-network algorithms in the service of weighted model counting. % We leave as future work leveraging advances in specialized hardware (e.g., using TPUs--Tensor Processing Units \cite{JYPPABBBBB17}).

\emph{Tensor networks} provide a natural bridge between high-performance computing and weighted model counting. Tensor networks are a tool used across computer science for reasoning about big-data processing, quantum systems, and more \cite{BB17,Cichocki14,Orus14}. A tensor network describes a complex tensor as a computation on many simpler tensors, and the problem of tensor-network \emph{contraction} is to perform this computation. Contraction is a key operation in neural network training and inference \cite{BK07,Hirata03,KKCLA17,VZTGDMVAC18}, and, as such, many of the optimizations for neural networks also apply to tensor-network contraction \cite{KSTKPPRS19,NRBHHJN15,RMGZFZHVL19}.

Chapter \ref{ch:tensors} demonstrated that tensor-network contraction can be used to perform weighted model counting using a 3-stage algorithm. First, in the \emph{reduction} stage, a counting instance is reduced to a tensor-network problem. Second, in the \emph{planning} stage, an order to contract tensors in the network is determined. Finally, in the \emph{execution} stage, tensors in the network are contracted according to the best discovered order. The resulting weighted model counter is useful as part of a portfolio of existing model counters when evaluated on a single core. In this chapter, we explore the impact of multiple-core and GPU use on tensor network contraction for weighted model counting. 

The planning stage in Chapter \ref{ch:tensors} was done using a choice of several single-core heuristic tree-decomposition solvers \cite{AMW17,HS18,Tamaki17}. There is little recent work on parallelizing heuristic tree-decomposition solvers. Instead, we implement a parallel portfolio of single-core tree-decomposition solvers and find that this portfolio significantly improves planning on multiple cores. Similar portfolio approaches have been well-studied and shown to be beneficial in the context of SAT solvers \cite{BSS15,XHHL08}. As a theoretical contribution, we prove that branch-decomposition solvers can also be included in this portfolio. Unfortunately, we find that a current branch-decomposition solver does not significantly improve the portfolio.

The execution stage in Chapter \ref{ch:tensors} was done using \tool{numpy} \cite{numpy} and evaluated on a single core. In this chapter, we add an implementation of the execution stage that uses \tool{TensorFlow} \cite{ABCCDDDGII16} to leverage a GPU for large contractions. Since GPU memory is significantly more limited than CPU memory, we add an implementation of \emph{index slicing}. Index slicing is a recent technique from the tensor-network community \cite{CZHNS18,GK20,VBNHRBM19}, analogous to the classic technique of conditioning in Bayesian Network reasoning \cite{darwiche01,dechter99,pearl86,SAS94}, that allows memory to be significantly reduced at the cost of additional time. We find that, while multiple cores do not significantly improve the contraction stage, a GPU provides significant speedup, especially when augmented with index slicing.

We implement our techniques in \tool{TensorOrder2}, a new parallel weighted model counter. We compare \tool{TensorOrder2} to a variety of state-of-the-art counters. We show that the improved \tool{TensorOrder2} is the fastest counter on 11\% of benchmarks after preprocessing \cite{LM14}, outperforming the GPU-based counter \tool{gpuSAT2} \cite{FHZ19}. Thus \tool{TensorOrder2} is useful as part of a portfolio of counters. All code and data are available at  \url{https://github.com/vardigroup/TensorOrder}.

The rest of the paper is organized as follows. 
% In Section~\ref{sec:prelim}, we give background information on weighted model counting, graph decompositions, and tensor networks. 
% In Section~\ref{sec:algorithm}, we discuss the algorithm for performing weighted model counting using tensor networks as outlined by \cite{DDV19}.
In Section~\ref{sec:parallel}, we describe parallelization of this algorithm and two related theoretical improvements: planning with branch decompositions, and index slicing. 
In Section~\ref{sec:experiments}, we implement this parallelization in \tool{TensorOrder2} and analyze its performance experimentally.

%both the machine learning and the high-performance computing communities.
% Many problems in artificial intelligence have significantly benefited from parallelization. 

%In this work, we aim to leverage the massive practical work in machine learning and high-performance computing on the parallelization of neural network inference 


%for weighted model counting, leveraging both multiple CPUs \cite{BSB15} and GPUs \cite{FHWZ18}.
% Most counting tools focus on single-core performance. Moreover, parallelizing such tools directly is challenging. For example, CDCL is known to be difficult to parallelize well. Instead, recent work designed a weighted model counting solver for a GPU from the ground up. 
%Even when the weight function is a constant function, constrained counting is \#P-Complete \cite{Valiant79}. Nevertheless, the development of tools that can successfully compute the total weight on large industrial formulas is an area of active research \cite{OD15,Thurley2006}. 

% Paragraph about work in TensorFlow

% A natural algorithmic framework to use is tensor networks \cite{google TN paper}. Tensor networks are great. Prior work showed they are promising for model counting.

% Constrained counting can be reduced to the problem of tensor-network contraction \cite{BMT15}. \emph{Tensor networks} are a tool used across quantum physics and computer science for describing and reasoning about quantum systems, big-data processing, and more \cite{BB17,Cichocki14,Orus14}. A tensor network describes a complex tensor as a computation on many simpler tensors, and the problem of tensor-network \emph{contraction} is to perform this computation. Although tensor networks can be seen as a variant of factor graphs \cite{KFL01}, working directly with tensor networks allows us to leverage massive practical work in machine learning and high-performance computing on tensor contraction \cite{BK07,Hirata03,KKCLA17,VZTGDMVAC18} (which also includes GPU support \cite{KSTKPPRS19,NRBHHJN15}) to perform constrained counting. Since tensor networks are relatively unknown in the artificial intelligence community, we give an introduction of relevant material on tensors and tensor networks in Section \ref{sec:tensors}.

% Contracting a tensor network requires determining an order to contract the tensors inside the network, and so efficient contraction requires finding a contraction order that minimizes computational cost. Since the number of possible contraction orders grows exponentially in the number of tensors, exhaustive algorithms, e.g. \cite{PHV14}, cannot scale to handle the large tensor networks required for the reduction from constrained counting. Instead, recent work \cite{KCMR18} gave heuristics that can sometimes find a ``good-enough'' contraction order. Finding efficient contraction orders for tensor networks remains an area of active research \cite{RTPCTSL19}.

% and discuss prior work on the optimization of tensor-network contraction in Section~\ref{sec:tensors}. We introduce a framework for solving the problem of weighted model counting with tensor networks in Section~\ref{sec:wmc}. We use branch decompositions to factor tensor networks in Section~\ref{sec:branch-preprocessing}. We present an experimental evaluation of tensor-based approaches to model counting in Section~\ref{sec:experiments}. Finally, we discuss future work and conclude in Section~\ref{sec:conclusion}.