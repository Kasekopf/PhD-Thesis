\emph{Tensor networks} are a tool used across quantum physics and computer science for describing and reasoning about quantum systems, big-data processing, and more \cite{BB17,Cichocki14,Orus19}.
A tensor network describes a complex tensor as a computation on many simpler tensors, and the problem of \emph{tensor-network contraction} is to perform this computation. 
Contraction is a key operation in neural network training and inference \cite{BK07,Hirata03,KKCLA17,VZTGDMVAC18}, and, as such, many of the optimizations for neural networks also apply to tensor-network contraction \cite{KSTKPPRS19,NRBHHJN15,RMGZFZHVL19}.
Moreover, there is a known reduction from unweighted model counting to tensor-network contraction \cite{BMT15}.
Tensor networks are therefore a natural bridge between high-performance computing and discrete integration, but this connection has not been fully explored.

In this chapter, we exploit this bridge to build \tool{TensorOrder}, a new tool for weighted model counting through tensor network contraction. \tool{TensorOrder} uses a 3-phase algorithm to perform weighted model counting. First, in the \emph{reduction} phase, a weighted model counting instance is reduced to a tensor-network problem using a novel reduction. Second, in the \emph{planning} phase, an order to contract tensors in the network is determined. Finally, in the \emph{execution} phase, tensors in the network are contracted with state-of-the-art tensor libraries using the determined order.

The key theoretical challenge of this approach lies in the planning phase: the determined contraction order should minimize the computational cost of the execution phase. Since the number of possible contraction orders grows exponentially in the number of tensors, cost-based exhaustive algorithms, e.g. \cite{PHV14}, cannot scale to handle the large tensor networks required for the reduction from constrained counting. Instead, recent work \cite{KCMR18} gave heuristics that can sometimes find a ``good-enough'' contraction order through structure-based optimization. Finding efficient contraction orders for tensor networks remains an area of active research \cite{RTPCTSL19}.

The primary theoretical contribution of this chapter is the application of heuristic graph-decomposition techniques to find efficient contraction orders for tensor networks. Algorithms based on graph decompositions have been successful across computer science \cite{GLST17,MPPV04}, and their success in practice relies on finding good decompositions of arbitrary graphs. This, along with several recent competitions \cite{DKTW18}, has spurred the development of a variety of heuristics and tools for efficiently finding graph decompositions \cite{AMW17,HS18,Tamaki17}. While we do not establish new parameterized complexity results for model counting (as fixed-parameter algorithms for model counting are well-known for a variety of parameters \cite{FMR08,SS10}), we combine these theoretical results with high-performance tensor network libraries and with existing heuristic graph-decomposition tools to produce a competitive tool for weighted model counting.

We first discuss the \textbf{Line-Graph} planner (\textbf{LG}) for finding efficient contraction orders through structure-based graph analysis. First applied to tensor networks by Markov and Shi \cite{MS08}, we contribute a new analysis that more closely matches the memory usage of existing tensor libraries. Our analysis combines two theoretical insights: (1) memory-efficient contraction orders are equivalent to low-width carving decompositions (first observed in \cite{de15}), and (2) tree decompositions can be used to find carving decompositions. \textbf{LG} has previously been implemented using exact tools for finding tree decompositions \cite{DFGHSW18}, but its implementation using heuristic tools for tree decompositions is largely unexplored.
% Moreover, our analysis implies that memory-optimal contraction orders for planar tensor networks can be found in cubic time.

Although \textbf{LG} is a general-purpose technique for finding contraction orders, \textbf{LG} cannot handle high-rank tensors and so cannot solve many existing counting benchmarks. We therefore contribute a novel structure-based method for finding efficient contraction orders, tailored for constrained counting: the \textbf{Factor-Tree} planner (\textbf{FT}). \textbf{FT} factors high-rank, highly-structured tensors as a preprocessing step, leveraging prior work on Hierarchical Tucker representations \cite{Grasedyck10}.

In order to compare our approaches against other counters (weighted model counters \tool{cachet} \cite{SBK05}, \tool{miniC2D} \cite{OD15}, and \tool{d4} \cite{LM17}, and unweighted model counters \tool{dynQBF} \cite{CW16}, \tool{dynasp} \cite{FHMW17}, and \tool{SharpSAT} \cite{Thurley2006}), as well as other tensor-based planning techniques, we implemented \textbf{LG} and \textbf{FT} in our new tool \tool{TensorOrder} using three state-of-the-art heuristic tree-decomposition solvers. \textbf{LG} outperforms other model counters and tensor-based planning techniques on a set of unweighted benchmarks, while \textbf{FT} improves the virtual best solver on 21\% of a standard set of weighted model counting benchmarks. \tool{TensorOrder} is thus useful as part of a portfolio of weighted model counters. All code, benchmarks, and detailed data of benchmark runs are available at \url{https://github.com/vardigroup/TensorOrder}.


The rest of the chapter is organized as follows: we introduce a framework for solving the problem of weighted model counting with tensor networks in Section~\ref{sec:tensors:algorithm}.
We discuss a novel reduction from weighted model counting to tensor-network contraction in Section \ref{sec:tensors:wmc}.
We discuss algorithms for contracting a tensor network in Section \ref{sec:tensors:execution}.
We discuss algorithms for constructing contraction trees in Section \ref{sec:tensors:planning}, including the \textbf{Line-Graph} planner and the \textbf{Factor-Tree} planner.
We implement these techniques in \tool{TensorOrder} and analyze its performance experimentally in Section \ref{sec:tensors:experiments}.
Finally, we conclude in Section~\ref{sec:tensors:conclusion}.
