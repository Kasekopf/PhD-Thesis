\section{Discussion}
In this work, we explored the impact of multiple-core and GPU use on tensor network contraction for weighted model counting. We implemented our techniques in \tool{TensorOrder2}, a new parallel counter, and showed that \tool{TensorOrder2} is useful as part of a portfolio of counters.

In the planning stage, we showed that a parallel portfolio of graph-decomposition solvers is significantly faster than single-core approaches. We proved that branch decomposition solvers can also be included in this portfolio, but concluded that a state-of-the-art branch decomposition solver only slightly improves the portfolio. For future work, it would be interesting to consider leveraging other width parameters (e.g. \cite{AGG07} or \cite{GS17}) in the portfolio as well. It may also be possible to improve the portfolio through more advanced algorithm-selection techniques \cite{HHLKS09,XHHL12}. One could develop parallel heuristic decomposition solvers directly, e.g., by adapting the exact GPU-based tree-decomposition solver \cite{VB17} into a heuristic solver.

In the execution stage, we showed that tensor contractions can be performed with \pkg{TensorFlow} on a GPU. When combined with index slicing, we concluded that a GPU speeds up the execution stage for many hard benchmarks. For easier benchmarks, the overhead of a GPU may outweigh any contraction speedups. We focused here on parallelism within a single tensor contraction, but there are opportunities in future work to exploit higher-level parallelism, e.g. by running each slice computation in Algorithm \ref{alg:tn-sliced} on a separate GPU. 

\pkg{TensorFlow} also supports performing tensor contractions on TPUs (tensor processing unit \cite{JYPPABBBBB17}), which are specialized hardware designed for neural network training and inference. Tensor networks therefore provide a natural framework to leverage TPUs for weighted model counting as well. There are additional challenges in the TPU setting: floating-point precision is limited, and there is a (100+ second) compilation stage from a contraction tree into XLA \cite{XLA}. We plan to explore these challenges further in future work.
