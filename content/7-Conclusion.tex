\chapter{Conclusion and Future Directions}
\label{ch:conclusion}
Just a rough outline for now, gathered from the various papers.

\section{Future Work}

\subsection{Formal Limitations}
It would be interesting in the future to analyze the types of benchmarks amenable to tensor-network methods, e.g. by computing lower bounds on carving width in addition to the upper bounds given by heuristic methods. 

\subsection{Improving Planning}
In the planning phase, the parallel portfolio of planners can be improved simply by integrating more planning tools. %TODO: Mention Hick's parallel branch decomposition tool? \cite{hicks2000branch}
It may also be possible to improve the portfolio through more advanced algorithm-selection techniques \cite{HHLKS09,XHHL12}. 
One could also develop parallel heuristic decomposition solvers directly, e.g., by adapting the exact GPU-based tree-decomposition solver \cite{VB17} into a heuristic solver. 

It would be interesting to consider leveraging other width parameters (e.g. \cite{AGG07} or \cite{GS17}) in the portfolio as well.

\subsection{Improving Execution}
We can also make the execution tool \Dmc{} support multicore ADD packages (\eg, \sylvan{} \cite{van2015sylvan}).

We focused here on parallelism within a single tensor contraction, but there are opportunities in future work to exploit higher-level parallelism. 
One promising approach is to run each slice computation in Algorithm \ref{alg:tn-sliced} on a separate CPU core, on a separate GPU, or even on entirely separate nodes in a cluster. %TODO: cite
Integrating slicing with the ADD-based execution from Chapter \ref{ch:dpmc} is an additional avenue for parallelization.


Whether or not sparse ADDs outperform tensors for richer hardware architectures as well is a subject for future work. %TODO: expand. ADDs are hard to do on other hardwares
We will also consider adding to our framework an executor based on databases (\eg, \cite{fichte2020exploiting}).


\subsection{Beyond CNF Formulas}
Tensor-based methods can also be used to count other classes of CSPs. For example, all techniques we introduced in this work would have similar performance computing the weighted model count of formulas that mix OR clauses with XOR clauses and Exactly-One clauses (as such clauses can also be represented as tree-factorable tensors). More generally, our algorithms for tensor-network contraction can be used to improve many other applications of tensor networks, e.g. quantum circuit simulation \cite{MS08}. Evaluating our techniques on a wider collection of tensor networks is an exciting direction for future work. % that may enable new applications of tensor networks.

\subsection{Beyond Discrete Integration}
Finally, decision diagrams have been widely used in artificial intelligence in the context of \emph{knowledge compilation}, where formulas are compiled into a tractable form in an early phase to support efficient query processing \cite{koriche2013knowledge,LM17,darwiche2004new,OD15}.
Our work opens up an investigation into the combination of knowledge compilation and dynamic programming.
The focus here is on processing a single model-counting query.
Exploring how dynamic programming can also be leveraged to handle several queries is another promising research direction.

In future work, \procount{} can be generalized for maximum model counting \cite{fremont2017maximum} and even other types of functional aggregate queries (FAQs) \cite{KNR16}, including MAP.

While the focus here is on model counting, our framework is of broader interest.
For example, in \cite{tabajara2017factored}, Tabajara and Vardi described a dynamic-programming, binary-decision-diagram-based framework for functional Boolean synthesis.
Refactoring the algorithm into a planning phase followed by an execution phase is also of interest in that context.


% \pkg{TensorFlow} also supports performing tensor contractions on TPUs (tensor processing unit \cite{JYPPABBBBB17}), which are specialized hardware designed for neural network training and inference. Tensor networks therefore provide a natural framework to leverage TPUs for weighted model counting as well. There are additional challenges in the TPU setting: floating-point precision is limited, and there is a (100+ second) compilation stage from a contraction tree into XLA \cite{XLA}. We plan to explore these challenges further in future work.

