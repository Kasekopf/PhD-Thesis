\thispagestyle{empty}
\begin{abstract}
Model counting is a fundamental problem in artificial intelligence, with applications in probabilistic reasoning, planning, inexact computing, engineering reliability, and statistical physics. The task is to count the total weight, subject to a given weight function, of the set of solutions of input constraints. The development of tools that can successfully compute the total weight on large industrial formulas is an area of active research.

Over the last ten years, hundreds of thousands of research hours have been poured into low-level computational tools and compilers for neural network training and inference. Simultaneously, there has been a surge in high-level reasoning tools based on graph decompositions, spurred by several recent competitions. While some existing model counters tightly integrate with either these low-level computational or high-level reasoning tools, no existing model counter is able to leverage both at the same time.

In my thesis, I demonstrate that a clean separation of high-level reasoning (\emph{planning}) and low-level computation (\emph{execution}) leads to scalable and more flexible model counters. Instead of building tightly on any particular tool, we target APIs that can be fulfilled by multiple implementations. This requires novel theoretical and algorithmic techniques to use existing high-level reasoning tools in a way consistent with the options available in popular low-level computational libraries. The resulting model counters perform well in many hardware settings (singlecore, multicore, GPU).
\end{abstract}


