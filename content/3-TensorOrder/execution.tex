\section{Execution Phase: Contracting the Tensor Network}
\label{sec:tensors:execution}
We focus in this work on tensor networks with relatively few (or no) free indices and hundreds or thousands of bond indices. Such tensor networks are obtained in a variety of applications \cite{Cichocki14,DLVR18}, including the reduction from model counting to tensor network contraction \cite{BMT15}. Although the rank of the contraction of the tensor network $N$ is small in this case, computing entries of $\tntensor{N}$ by
directly following Equation \ref{eqn:contraction} requires performing a summation over an exponential number of terms--- one for each assignment in $\domain{\tnbound{N}}$--- and is therefore infeasible.

$\tntensor{N}$ can instead be computed by recursively decomposing the tensor network, as in Algorithm \ref{alg:network-contraction} \cite{EP14}. The choice of contraction tree $T$ does not affect the output of Algorithm \ref{alg:network-contraction} but may have a dramatic impact on the running-time and memory usage. We explore this in more detail in the following section.

\begin{algorithm*}[t]
    \label{alg:network-contraction}
    \caption{Recursively contracting a tensor network}
    
    \DontPrintSemicolon
    \KwIn{A tensor network $N$}
    \KwIn{A contraction tree $T$}
    \KwOut{$\tntensor{N}$, the contraction of $N$ as given in Definition \ref{def:contraction}}
    \Function{\upshape $\func{Contract}(N, T)$}{
        \If{$\left|N\right| = 1$} {
            \Return{$\textup{the tensor contained in}~N$}
        }\Else{
            $T_1, T_2 \gets \text{immediate subtrees of}~T$\;
            $A_1 \gets \func{Contract}(\Lv{T_1}, T_1)$\;
            $A_2 \gets \func{Contract}(\Lv{T_2}, T_2)$\;
            \Return{$A_1 \cdot A_2$}
        }
    }
\end{algorithm*}
